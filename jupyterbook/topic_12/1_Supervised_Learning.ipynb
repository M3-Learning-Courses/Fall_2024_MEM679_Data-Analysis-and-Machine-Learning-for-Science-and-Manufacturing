{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Introduction to Linear Machine Learning\n",
    "\n",
    "![](figs/big_hammer.jpeg)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "from IPython.display import IFrame\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib\n",
    "matplotlib.rcParams.update({'font.size': 12})\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import ListedColormap\n",
    "\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import cross_val_score, train_test_split\n",
    "from sklearn import neighbors, datasets, svm\n",
    "from sklearn.datasets import load_breast_cancer, load_iris, make_blobs, make_circles\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.compose import make_column_selector as selector\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.ensemble import HistGradientBoostingClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## What is Machine Learning?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Everyone write down their definition\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- “Machine Learning at its most basic is the practice of using algorithms to parse data, learn from it, and then make a determination or prediction about something in the world.” – Nvidia\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- “Machine learning is based on algorithms that can learn from data without relying on rules-based programming.”- McKinsey & Co.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- “The field of Machine Learning seeks to answer the question “How can we build computer systems that automatically improve with experience, and what are the fundamental laws that govern all learning processes?” – Carnegie Mellon University\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Machine learning research is part of research on artificial intelligence, seeking to provide knowledge to computers through data, observations and interacting with the world. That acquired knowledge allows computers to correctly generalize to new settings.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- “Machine Learning is the science of getting computers to learn and act like humans do, and improve their learning over time in autonomous fashion, by feeding them data and information in the form of observations and real-world interactions.”\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Mathematical Definition\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "The problem is to take _n-samples_ of data with _m-features_ and try to predict a property of unknown data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Generally, each sample has multiple attributes commonly called **features**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- When using linear machine learning models choice of features is essential\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Machine Learning Basic Concepts\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Representations\n",
    "\n",
    "- A set of classifiers or the language that a computer understands\n",
    "- How the model connects data to the objective\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "**The way humans make inferences from data is different than machines**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Examples:\n",
    "\n",
    "- K-nearest neighbors\n",
    "- support vector machines\n",
    "- decision trees\n",
    "- neural networks\n",
    "- ...\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Evaluation\n",
    "\n",
    "- how the model determines its success at completing an objective\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "**The ways humans and computers quantify success at **an objective **are** very different\\*\\*\\*\\*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Examples:\n",
    "\n",
    "- accuracy/error rate\n",
    "- squared error\n",
    "- likelihood\n",
    "- KL divergence (entropy between two distributions)\n",
    "- ...\n",
    "\n",
    "**Note for Engineers - The concepts of statistical thermodynamics are very relivant to machine learning dynamics**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Optimization\n",
    "\n",
    "- the model search method\n",
    "- how the model improves itself\n",
    "- how the model values exploration vs. exploitation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "**The way that humans and computers optimize and solve problems is very different**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Examples:\n",
    "Combinatorial optimization\n",
    "\n",
    "- random search\n",
    "- greedy search\n",
    "  Continuous optimization\n",
    "- gradient descent\n",
    "- quasi-Newton method\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "![figure](figs/Componets_of_a_learning_algorithm.png)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "IFrame(src='http://www.r2d3.us/visual-intro-to-machine-learning-part-1/', width=1000, height=800)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## How do we get machines to learn?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Choose the best learning algorithm\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Other things that matter\n",
    "\n",
    "- Collect and collate meaningful data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Provide the data to the machine in a form that emphasizes the learning objective\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Humans and machines are great collaborators:\n",
    "\n",
    "- Humans are great at deducing conclusions from a few examples based on abstract connections between observations\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Machines are great at searching large high-dimensional information for statistical trends\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Challenges and Limitations\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Overfitting\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "![figure](figs/Explaining_Overfitting.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "This might be a comfy bed for you, but I don't know if your friend would like to sleep on it ☺️\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- It is possible to get perfect accuracy on a test set but cannot conduct inference on a new problem\n",
    "- The model is not **generalizable**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "A model can classify types of apples but show it an orange and it is useless\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Overfitting Example with Polynomials\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Defines the function\n",
    "def true_fun(X):\n",
    "    return np.cos(1.5 * np.pi * X)\n",
    "\n",
    "\n",
    "# sets a random seed for consistent plotting\n",
    "np.random.seed(0)\n",
    "\n",
    "# sets the number of samples\n",
    "n_samples = 100\n",
    "\n",
    "# Sets the range in degrees\n",
    "degrees = [1, 4, 15]\n",
    "\n",
    "# adds some noise to the data\n",
    "X = np.sort(np.random.rand(n_samples))\n",
    "y = true_fun(X) + np.random.randn(n_samples) * 0.3\n",
    "\n",
    "# does the plotting\n",
    "plt.figure(figsize=(14, 5))\n",
    "\n",
    "# Loops around the number of degrees selected\n",
    "for i in range(len(degrees)):\n",
    "\n",
    "    # makes the subplot\n",
    "    ax = plt.subplot(1, len(degrees), i + 1)\n",
    "    plt.setp(ax, xticks=(), yticks=())\n",
    "\n",
    "    # creates the polynomial\n",
    "    polynomial_features = PolynomialFeatures(degree=degrees[i],\n",
    "                                             include_bias=False)\n",
    "\n",
    "    # Least squares linear regression\n",
    "    linear_regression = LinearRegression()\n",
    "\n",
    "    # establishes a fitting pipeline\n",
    "    pipeline = Pipeline([(\"polynomial_features\", polynomial_features),\n",
    "                         (\"linear_regression\", linear_regression)])\n",
    "\n",
    "    # does the fit\n",
    "    pipeline.fit(X[:, np.newaxis], y)\n",
    "\n",
    "    # Evaluate the models using crossvalidation\n",
    "    scores = cross_val_score(pipeline, X[:, np.newaxis], y,\n",
    "                             scoring=\"neg_mean_squared_error\", cv=10)\n",
    "\n",
    "    # Defines a linear vector\n",
    "    X_test = np.linspace(0, 1, 100)\n",
    "\n",
    "    # plots the real model\n",
    "    plt.plot(X_test, pipeline.predict(X_test[:, np.newaxis]), label=\"Model\")\n",
    "    plt.plot(X_test, true_fun(X_test), label=\"True function\")\n",
    "\n",
    "    # plots the generated data\n",
    "    plt.scatter(X, y, edgecolor='b', s=20, label=\"Samples\")\n",
    "\n",
    "    # sets the axes format\n",
    "    plt.xlabel(\"x\")\n",
    "    plt.ylabel(\"y\")\n",
    "    plt.xlim((0, 1))\n",
    "    plt.ylim((-2, 2))\n",
    "    plt.legend(loc=\"best\")\n",
    "    plt.title(\"Degree {}\\nMSE = {:.2e}(+/- {:.2e})\".format(\n",
    "        degrees[i], -scores.mean(), scores.std()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Categories of Machine Learning\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "### Supervised Learning\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- The data that we have comes with examples that have ground truths for what we want to predict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "IFrame(src='https://scikit-learn.org/stable/supervised_learning.html#supervised-learning', width=1000, height=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Classification\n",
    "\n",
    "- Samples belong to two or more classes ... we want to learn from labeled data how to predict the class of an unlabeled example.\n",
    "  - Example: Read handwritten digits from images\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "### Regression\n",
    "\n",
    "- The desired output consists of one or more continuous variables.\n",
    "  - Example: Predict the yield of product for a given operating condition\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Unsupervised Learning\n",
    "\n",
    "- Training data consists of input values without any target values\n",
    "  Goals are to:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- **Cluster** - Cluster data into groups\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- **Dimensionality Reduction** - Reduce the dimensionality of data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- **Generation** - Try to generate new samples that belong to the same distribution\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "IFrame(src='https://scikit-learn.org/stable/unsupervised_learning.html#unsupervised-learning', width=1000, height=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Designing a Training and Validation\n",
    "\n",
    "### Training and Testing Sets\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- When conducting machine learning we want to train the model with one **training dataset** and then apply it to new never-before-seen **testing data**\n",
    "- It is quite common to have a **validation dataset** that you can validate your model independent of your in-training metrics\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Example Workflow\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "### Loading Example Datasets\n",
    "\n",
    "- `scikit-learn` has a handful of datasets for testing purposes\n",
    "- It is always good to benchmark models on standard data sets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# imports iris dataset\n",
    "iris = datasets.load_iris()\n",
    "\n",
    "# imports digits dataset\n",
    "digits = datasets.load_digits()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Each of these datasets are classes that contain:\n",
    "  - `*.data` : `n_samples, n_features` samples and features\n",
    "  - `*.target` : ground truth\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Visualization\n",
    "\n",
    "You must have an idea of what the data from a random sample of your dataset looks like\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# selects a random example\n",
    "ind = np.random.randint(0,100)\n",
    "\n",
    "plt.imshow(digits.data[ind].reshape(8,8))\n",
    "plt.title(f'Target is {digits.target[ind]}')\n",
    "plt.tick_params(left = False, right = False , labelleft = False ,\n",
    "                labelbottom = False, bottom = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Learning to Fit and Predict\n",
    "\n",
    "- For now, we can think of the algorithm as a semi-perfect magical black box\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# makes an object that is the classifier with specific hyperparameter\n",
    "# this is the model that you are training\n",
    "clf = svm.SVC(gamma=0.001, C=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Here we choose some good parameters that might have to use gridsearch or cross-validation to determine these\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "### Fitting the data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Fit the model using some data\n",
    "# Includes all data except the last index\n",
    "clf.fit(digits.data[:-1], digits.target[:-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "### Visualizing the Results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# selects a random example\n",
    "ind = np.random.randint(0,100)\n",
    "\n",
    "plt.imshow(digits.data[ind].reshape(8,8))\n",
    "plt.title(f'Target is {digits.target[ind]}, the prediction is {clf.predict(digits.data[ind].reshape(1,-1))}')\n",
    "plt.tick_params(left = False, right = False , labelleft = False ,\n",
    "                labelbottom = False, bottom = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Saving a model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Once trained it is possible to save a model to a file using `pickle`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "#### Saves the Model\n",
    "\n",
    "Saves the model object as a pickle\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# saves the model \n",
    "s = pickle.dumps(clf)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "#### Loads the Saved Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# loads the model\n",
    "clf2 = pickle.loads(s)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "#### Tests the Loaded Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# test the model\n",
    "# selects a random example\n",
    "ind = np.random.randint(0,100)\n",
    "\n",
    "plt.imshow(digits.data[ind].reshape(8,8))\n",
    "plt.title(f'Target is {digits.target[ind]}, the prediction is {clf2.predict(digits.data[ind].reshape(1,-1))}')\n",
    "plt.tick_params(left = False, right = False , labelleft = False ,\n",
    "                labelbottom = False, bottom = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Types of Objects in Scikit-Learn\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "1. **Datasets:** List of multidimensional observations\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "2. **Estimator Objects:** Any object that learns from data.\n",
    "   In estimators you can: - Set parameters `estimator = Estimator(param1=1, param2=2)` - Fit some data `estimator.fit(data)` - Predict the output from some data `estimator.predict(data)`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Supervised Learning - Details\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "### Example: Classifying Irises\n",
    "\n",
    "The iris dataset is a classification task consisting in identifying 3 different types of irises (Setosa, Versicolour, and Virginica) from their petal and sepal length and width:\n",
    "\n",
    "We will use this very simple dataset to explain ML concepts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# loads iris dataset\n",
    "iris = datasets.load_iris()\n",
    "\n",
    "# Sets the data to iris_x\n",
    "iris_X = iris.data\n",
    "\n",
    "# Sets the target to iris_y\n",
    "iris_y = iris.target\n",
    "\n",
    "# shows the unique targets\n",
    "np.unique(iris_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## K-Nearest Neighbor Classifier\n",
    "\n",
    "- One of the most simple classifiers\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Finds a predefined number of training samples (defined by the user) to a new point for classification\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "![](figs/KNN-Classification.gif)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Can use any distance metric but most commonly Euclidean distance\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# sets the number of nearest neighbors\n",
    "n_neighbors = 15\n",
    "\n",
    "# import the iris data\n",
    "iris = datasets.load_iris()\n",
    "\n",
    "# we only take the first two features.\n",
    "# slicing by using a two-dim dataset\n",
    "X = iris.data[:, :2]\n",
    "y = iris.target\n",
    "\n",
    "h = .02  # step size in the mesh\n",
    "\n",
    "# Create color maps\n",
    "cmap_light = ListedColormap(['#FFAAAA', '#AAFFAA', '#AAAAFF'])\n",
    "cmap_bold = ListedColormap(['#FF0000', '#00FF00', '#0000FF'])\n",
    "\n",
    "for weights in ['uniform', 'distance']:\n",
    "    # we create an instance of Neighbors Classifier and fit the data.\n",
    "    clf = neighbors.KNeighborsClassifier(n_neighbors, weights=weights)\n",
    "    clf.fit(X, y)\n",
    "\n",
    "    # Plot the decision boundary. For that, we will assign a color to each\n",
    "    # point in the mesh [x_min, x_max]x[y_min, y_max].\n",
    "    x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
    "    y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
    "    xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n",
    "                         np.arange(y_min, y_max, h))\n",
    "    Z = clf.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "\n",
    "    # Put the result into a color plot\n",
    "    Z = Z.reshape(xx.shape)\n",
    "    plt.figure()\n",
    "    plt.pcolormesh(xx, yy, Z, cmap=cmap_light)\n",
    "\n",
    "    # Plot also the training points\n",
    "    plt.scatter(X[:, 0], X[:, 1], c=y, cmap=cmap_bold,\n",
    "                edgecolor='k', s=20)\n",
    "    plt.xlim(xx.min(), xx.max())\n",
    "    plt.ylim(yy.min(), yy.max())\n",
    "    plt.title(\"3-Class classification (k = %i, weights = '%s')\"\n",
    "              % (n_neighbors, weights))\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- 'uniform' : uniform weights. All points in each neighborhood are weighted equally.\n",
    "- 'distance' : weight points by the inverse of their distance. In this case, closer neighbors of a query point will have a greater influence than neighbors who are further away.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Curse of Dimensionality\n",
    "\n",
    "- As the dimensionality of a dataset grows it becomes sparse\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- For an estimator to be reasonable you need the distance between points to be less than some value _d_ which depends on the problem\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- The amount of data needed (to be statistically significant) grows exponentially as the dimensionality increases\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "### Excluding one dimension in the classification\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import ListedColormap\n",
    "from sklearn import neighbors, datasets\n",
    "\n",
    "n_neighbors = 15\n",
    "\n",
    "# import some data to play with\n",
    "iris = datasets.load_iris()\n",
    "\n",
    "# we only take the first two features.\n",
    "# slicing by using a two-dim dataset\n",
    "X = iris.data[:, :2]\n",
    "y = iris.target\n",
    "\n",
    "h = .02  # step size in the mesh\n",
    "\n",
    "# Create color maps\n",
    "cmap_light = ListedColormap(['#FFAAAA', '#AAFFAA', '#AAAAFF'])\n",
    "cmap_bold = ListedColormap(['#FF0000', '#00FF00', '#0000FF'])\n",
    "\n",
    "for weights in ['uniform', 'distance']:\n",
    "    # we create an instance of Neighbors Classifier and fit the data.\n",
    "    clf = neighbors.KNeighborsClassifier(n_neighbors, weights=weights)\n",
    "    clf.fit(X, y)\n",
    "\n",
    "    # Plot the decision boundary. For that, we will assign a color to each\n",
    "    # point in the mesh [x_min, x_max]x[y_min, y_max].\n",
    "    x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
    "    y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
    "    xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n",
    "                         np.arange(y_min, y_max, h))\n",
    "    Z = clf.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "\n",
    "    # Put the result into a color plot\n",
    "    Z = Z.reshape(xx.shape)\n",
    "    plt.figure()\n",
    "    plt.pcolormesh(xx, yy, Z, cmap=cmap_light)\n",
    "\n",
    "    # Plot also the training points\n",
    "    plt.scatter(X[:, 0], X[:, 1], c=y, cmap=cmap_bold,\n",
    "                edgecolor='k', s=20)\n",
    "    plt.xlim(xx.min(), xx.max())\n",
    "    plt.ylim(yy.min(), yy.max())\n",
    "    plt.title(\"3-Class classification (k = %i, weights = '%s')\"\n",
    "              % (n_neighbors, weights))\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- In 1D this requires $n \\propto 1/d$ however when you have _p_ features this becomes $n \\propto 1/d^{p}$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "**It is common to use ML to reduce the dimensionality of the problem**\n",
    "\n",
    "- This could be using dimensionality reduction techniques\n",
    "- Could be using featurization methods\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Using Machine Learning to Reduce the Curse of Dimensionality\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "### Linear Regression\n",
    "\n",
    "$y = x\\beta + \\epsilon$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "#### Example Dataset\n",
    "\n",
    "The diabetes dataset consists of 10 physiological variables (age, sex, weight, blood pressure) measured on 442 patients, and an indication of disease progression after one year:\n",
    "\n",
    "Want to predict disease progression\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "diabetes = datasets.load_diabetes()\n",
    "diabetes_X_train = diabetes.data[:-20]\n",
    "diabetes_X_test  = diabetes.data[-20:]\n",
    "diabetes_y_train = diabetes.target[:-20]\n",
    "diabetes_y_test  = diabetes.target[-20:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn import linear_model\n",
    "regr = linear_model.LinearRegression()\n",
    "regr.fit(diabetes_X_train, diabetes_y_train)\n",
    "\n",
    "print(regr.coef_)\n",
    "\n",
    "# The mean square error\n",
    "np.mean((regr.predict(diabetes_X_test) - diabetes_y_test)**2)\n",
    "\n",
    "# Explained variance score: 1 is perfect prediction\n",
    "# and 0 means that there is no linear relationship\n",
    "# between X and y.\n",
    "print(regr.score(diabetes_X_test, diabetes_y_test) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "**Clearly, a linear model does not work that well in this high-dimensional problem**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Shrinkage\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "**Problem:** if there are only a few data points per dimension, noise in the observation induces a large variance\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# makes example data\n",
    "X = np.c_[ .5, 1].T\n",
    "y = [.5, 1]\n",
    "\n",
    "# makes test data\n",
    "test = np.c_[ 0, 2].T\n",
    "\n",
    "# makes an object for a linear fit\n",
    "regr = linear_model.LinearRegression()\n",
    "\n",
    "# makes the figure\n",
    "plt.figure() \n",
    "\n",
    "# fixes the seed\n",
    "np.random.seed(0)\n",
    "\n",
    "# adds variance and does the fitting for samples with variance\n",
    "for _ in range(6): \n",
    "    this_X = .1 * np.random.normal(size=(2, 1)) + X\n",
    "    regr.fit(this_X, y)\n",
    "    plt.plot(test, regr.predict(test)) \n",
    "    plt.scatter(this_X, y, s=3)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "The way to deal with variance is to reduce the model complexity by adding statistical damaging mechanisms called **regularization**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### A Step back with simple mathematics\n",
    "\n",
    "We are fitting some data to a linear model <br>\n",
    "$\\hat{y} = w[0] \\times X[0] + w[1] \\times X[1] + ... + w[n] \\times X[n] + b$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "When we fit the cost function is the mean squared error:\n",
    "\n",
    "$\\sum_{i=1}^{M}(y_i-\\hat{y_i})^2 = \\sum_{i=1}^{M}(y_i - \\sum_{j=0}^{p}w_j\\times x_{ij})^2$ <br>\n",
    "M - number of instances or samples <br>\n",
    "p - number of features\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- If you do a test/train split you get an idea if the model is overfitting\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- We want to add constraints on the model which reduces the complexity (regularization)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Ridge Regression\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- The cost function is altered by adding a penalty equivalent to the square of the magnitude of the coefficients\n",
    "- This drives the coefficients to be small\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "##### The Cost Function for Ridge Regression\n",
    "\n",
    "$\\sum_{i=1}^{M}(y_i-\\hat{y_i})^2 = \\sum_{i=1}^{M}(y_i - \\sum_{j=0}^{p}w_j\\times x_{ij})^2 + \\lambda\\sum_{j=0}^{p}w_{j}^{2}$ <br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- shrinks the coefficients and multicollinearity\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- $\\lambda$ sets the magnitude of this effect\n",
    "- As $\\lambda \\rightarrow 0$ becomes a linear fit\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Examples\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "##### Loads data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "california=datasets.fetch_california_housing()\n",
    "california_df=pd.DataFrame(california.data,columns=california.feature_names)\n",
    "print(california_df.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "##### Adds the target into the pandas dataframe\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# add another column that contains the house prices which in scikit learn datasets are considered as target\n",
    "california_df['Price']=california.target\n",
    "print(california_df.head(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "##### Saves a new df without the price\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "newX=california_df.drop('Price',axis=1)\n",
    "print(newX[0:3]) # check \n",
    "newY=california_df['Price']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "##### Test Train Split\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "X_train,X_test,y_train,y_test=train_test_split(newX,newY,test_size=0.3,random_state=3)\n",
    "print(f'Test size: {len(X_test)} , {len(y_test)}')\n",
    "print(f'Train size: {len(X_train)} , {len(y_train)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "##### Builds the Linear Regression Object\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "lr = LinearRegression()\n",
    "lr.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "##### Builds the Ridge Regression Object\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "rr = Ridge(alpha=.01) # higher the alpha value, more restriction on the coefficients; low alpha > more generalization, coefficients are barely\n",
    "# restricted and in this case linear and ridge regression resembles\n",
    "rr.fit(X_train, y_train)\n",
    "\n",
    "rr100 = Ridge(alpha=1000) #  comparison with alpha value\n",
    "rr100.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "##### Evaluation of the Ridge Regression Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "train_score=lr.score(X_train, y_train)\n",
    "test_score=lr.score(X_test, y_test)\n",
    "Ridge_train_score = rr.score(X_train,y_train)\n",
    "Ridge_test_score = rr.score(X_test, y_test)\n",
    "Ridge_train_score100 = rr100.score(X_train,y_train)\n",
    "Ridge_test_score100 = rr100.score(X_test, y_test)\n",
    "\n",
    "\n",
    "print(\"linear regression train score:\", train_score)\n",
    "print(\"linear regression test score:\", test_score)\n",
    "print(\"ridge regression train score low alpha:\", Ridge_train_score)\n",
    "print(\"ridge regression test score low alpha:\", Ridge_test_score)\n",
    "print(\"ridge regression train score high alpha:\", Ridge_train_score100)\n",
    "print(\"ridge regression test score high alpha:\", Ridge_test_score100)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "##### Visualizing the results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "plt.plot(rr.coef_,alpha=0.7,linestyle='none',marker='*',markersize=5,color='red',label=r'Ridge; $\\alpha = 0.01$',zorder=7) # zorder for ordering the markers\n",
    "plt.plot(rr100.coef_,alpha=0.5,linestyle='none',marker='d',markersize=6,color='blue',label=r'Ridge; $\\alpha = 1000$') # alpha here is for transparency\n",
    "plt.plot(lr.coef_,alpha=0.4,linestyle='none',marker='o',markersize=7,color='green',label='Linear Regression')\n",
    "plt.xlabel('Coefficient Index',fontsize=16)\n",
    "plt.ylabel('Coefficient Magnitude',fontsize=16)\n",
    "plt.legend(fontsize=13,loc=4)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "### What does this mean?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- X-axis we have the coefficients for the 8 features\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- When $\\alpha$ is low looks like a linear fit\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- When $\\alpha$ is high all of the coefficients shrink\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "The model is less responsive to small variances in a particular parameter\n",
    "\n",
    "- Better at generalizing\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- The difference between the training and validation accuracy with a high $\\alpha$ is less, this means there is less overfitting.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "**Note** Ridge regression is regularly called **L2**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Lasso Regression\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "#### Cost Function for Lasso Regression\n",
    "\n",
    "$\\sum_{i=1}^{M}(y_i-\\hat{y_i})^2 = \\sum_{i=1}^{M}(y_i - \\sum_{j=0}^{p}w_j\\times x_{ij})^2 + \\lambda\\sum_{j=0}^{p}|w_{j}|$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Just like Ridge regression:\n",
    "\n",
    "- $\\lambda$ sets the magnitude of this effect\n",
    "- As $\\lambda \\rightarrow 0$ becomes a linear fit\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Lasso or L1 regularization drives the weights to 0:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Can be used to impose sparsity (independence) in model $\\rightarrow$ used in feature selection\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Lasso regression helps reduced overfitting\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Example\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "##### Loads the data and makes test train split\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "cancer = load_breast_cancer()\n",
    "print(cancer.keys())\n",
    "\n",
    "cancer_df = pd.DataFrame(cancer.data, columns=cancer.feature_names)\n",
    "print(cancer_df.head(3))\n",
    "\n",
    "X = cancer.data\n",
    "Y = cancer.target\n",
    "X_train,X_test,y_train,y_test=train_test_split(X,Y, test_size=0.3, random_state=31)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "##### Defines the model and prints results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# alpha = 1\n",
    "\n",
    "# instantiates the lasso model\n",
    "lasso = Lasso()\n",
    "# fits the training data\n",
    "lasso.fit(X_train,y_train)\n",
    "# computes the training score\n",
    "train_score=lasso.score(X_train,y_train)\n",
    "# computes the test score\n",
    "test_score=lasso.score(X_test,y_test)\n",
    "# counts the number of features used by the model\n",
    "coeff_used = np.sum(lasso.coef_!=0)\n",
    "\n",
    "# prints the results\n",
    "print(\"training score:\", train_score) \n",
    "print(\"test score: \", test_score)\n",
    "print (\"number of features used: \", coeff_used)\n",
    "\n",
    "# alpha = 0.01\n",
    "\n",
    "# instantiates the lasso model\n",
    "lasso001 = Lasso(alpha=0.01, max_iter=int(10e5))\n",
    "# fits the training data\n",
    "lasso001.fit(X_train,y_train)\n",
    "# computes the score\n",
    "train_score001=lasso001.score(X_train,y_train)\n",
    "test_score001=lasso001.score(X_test,y_test)\n",
    "# counts the number of features used by the model\n",
    "coeff_used001 = np.sum(lasso001.coef_!=0)\n",
    "\n",
    "# prints the results\n",
    "print( \"training score for alpha=0.01:\", train_score001 )\n",
    "print( \"test score for alpha =0.01: \", test_score001)\n",
    "print( \"number of features used: for alpha =0.01:\", coeff_used001)\n",
    "\n",
    "# alpha = 0.0001\n",
    "\n",
    "lasso00001 = Lasso(alpha=0.0001, max_iter=int(10e5))\n",
    "lasso00001.fit(X_train,y_train)\n",
    "train_score00001=lasso00001.score(X_train,y_train)\n",
    "test_score00001=lasso00001.score(X_test,y_test)\n",
    "coeff_used00001 = np.sum(lasso00001.coef_!=0)\n",
    "\n",
    "print( \"training score for alpha=0.0001:\", train_score00001 )\n",
    "print( \"test score for alpha =0.0001: \", test_score00001)\n",
    "print( \"number of features used: for alpha =0.0001:\", coeff_used00001)\n",
    "\n",
    "# linear regression\n",
    "\n",
    "lr = LinearRegression()\n",
    "lr.fit(X_train,y_train)\n",
    "lr_train_score=lr.score(X_train,y_train)\n",
    "lr_test_score=lr.score(X_test,y_test)\n",
    "\n",
    "print( \"LR training score:\", lr_train_score )\n",
    "print( \"LR test score: \", lr_test_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "f = plt.figure(figsize=(10,6))\n",
    "plt.subplot(1,2,1)\n",
    "plt.plot(lasso.coef_,alpha=0.7,linestyle='none',marker='*',markersize=5,color='red',label=r'Lasso; $\\alpha = 1$',zorder=7) # alpha here is for transparency\n",
    "plt.plot(lasso001.coef_,alpha=0.5,linestyle='none',marker='d',markersize=6,color='blue',label=r'Lasso; $\\alpha = 0.01$') # alpha here is for transparency\n",
    "\n",
    "plt.xlabel('Coefficient Index',fontsize=16)\n",
    "plt.ylabel('Coefficient Magnitude',fontsize=16)\n",
    "plt.legend(fontsize=13,loc=4)\n",
    "plt.subplot(1,2,2)\n",
    "plt.plot(lasso.coef_,alpha=0.7,linestyle='none',marker='*',markersize=5,color='red',label=r'Lasso; $\\alpha = 1$',zorder=7) # alpha here is for transparency\n",
    "plt.plot(lasso001.coef_,alpha=0.5,linestyle='none',marker='d',markersize=6,color='blue',label=r'Lasso; $\\alpha = 0.01$') # alpha here is for transparency\n",
    "plt.plot(lasso00001.coef_,alpha=0.8,linestyle='none',marker='v',markersize=6,color='black',label=r'Lasso; $\\alpha = 0.00001$') # alpha here is for transparency\n",
    "plt.plot(lr.coef_,alpha=0.7,linestyle='none',marker='o',markersize=5,color='green',label='Linear Regression',zorder=2)\n",
    "plt.xlabel('Coefficient Index',fontsize=16)\n",
    "plt.ylabel('Coefficient Magnitude',fontsize=16)\n",
    "plt.legend(fontsize=13,loc=4)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "##### Explaining the Results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- For the default case $\\alpha = 1$, 4 out of 30 features have non-zero coefficients\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "**Questions: is this model overfitting or underfitting?**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Both training and test results are low $\\rightarrow$ model is underfitting\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- $\\alpha = 0.01$, 10 out of 30 features have non-zero coefficients, and test and training scores increase\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- as $\\alpha$ decreases further resemble linear regression\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Visual Explanation of Lasso and Ridge Regression\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "![](figs/Lasso_ridge.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Logistic Regression\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Not all problems are continuous\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- There are instances where we want to determine a binary state\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Logistic regression is good for qualitative results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Logistic regression is concerned with the probability that a response falls into a particular category\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "### Evaluating Classification\n",
    "\n",
    "- True Positives\n",
    "- True Negatives\n",
    "- False Positives\n",
    "- False Negatives\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Logistic regression uses a Sigmoid function for situations where outputs can have one of two values either 0 or 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "$$\\omega(z)=\\frac{1}{1+e^{-z}}$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return (1/(1+np.exp(-x)))\n",
    "\n",
    "x = np.linspace(-6,6,100)\n",
    "plt.plot(x,sigmoid(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "### Take our linear regression solution and put it into a logistic model\n",
    "\n",
    "$$\\omega(z)=\\frac{1}{1+e^{-(x\\beta + \\epsilon)}}$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "### Example\n",
    "\n",
    "The Challenge:\n",
    "\n",
    "The sinking of the Titanic is one of the most infamous shipwrecks in history.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "On April 15, 1912, during her maiden voyage, the widely considered “unsinkable” RMS Titanic sank after colliding with an iceberg. Unfortunately, there weren’t enough lifeboats for everyone onboard, resulting in the death of 1502 out of 2224 passengers and crew.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "While there was some element of luck involved in surviving, it seems some groups of people were more likely to survive than others.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "In this challenge, we ask you to build a predictive model that answers the question: “what sorts of people were more likely to survive?” using passenger data (ie name, age, gender, socio-economic class, etc).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Use pandas to read in csv file\n",
    "train = pd.read_csv('./data/titanic/train.csv')\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "train.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Locates and Visualizes the Missing Values\n",
    "\n",
    "- You cannot train a machine learning model with missing values. It will not know what to do.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Use the .isnull() method to locate missing data\n",
    "missing_values = train.isnull()\n",
    "missing_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Use seaborn to conduct heatmap to identify missing data\n",
    "# data -> argument refers to the data to create heatmap\n",
    "# yticklabels -> argument avoids plotting the column names\n",
    "# cbar -> argument identifies if a colorbar is required or not\n",
    "# cmap -> argument identifies the color of the heatmap\n",
    "sns.heatmap(data = missing_values, yticklabels=False, cbar=False, cmap='viridis')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "#### Viewing the ratios of the labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Use the countplot() method to identify ratio of who survived vs. not\n",
    "# (Tip) very helpful to get a visualization of the target label\n",
    "# x -> argument refers to column of interest\n",
    "# data -> argument refers to dataset\n",
    "sns.countplot(x='Survived', data=train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "#### Survival Based on Catagory\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Use the countplot() method to identify ratio of who survived vs. not with interest in Passenger class\n",
    "# x -> argument refers to column of interest\n",
    "# data -> argument refers to dataset\n",
    "# hue -> allows another level to subdivide data\n",
    "# palette -> argument refers to plot color\n",
    "sns.countplot(x='Survived', data=train, hue='Pclass')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "##### What to do with all this missing data?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "We can impute or infill these values based on the data distribution\n",
    "\n",
    "- When doing this it is important to look at the data distribution to make sure this makes sense\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,7))\n",
    "sns.violinplot(x='Pclass', y='Age', data = train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "##### Create a function to Impute Missing Values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Create function to impute the age value if it is null\n",
    "def impute_age(cols):\n",
    "    Age = cols[0]\n",
    "    Pclass = cols[1]\n",
    "    \n",
    "    if pd.isnull(Age):\n",
    "        \n",
    "        if Pclass == 1:\n",
    "            return 37\n",
    "        elif Pclass == 2:\n",
    "            return 29\n",
    "        else:\n",
    "            return 24\n",
    "    else:\n",
    "        return Age\n",
    "    \n",
    "    \n",
    "# Apply function to impute the age for missing values\n",
    "# The age column is at position 0\n",
    "# The pclass column is at position 1\n",
    "# axis -> argument refers to columns instead of rows to apply the impute_age function\n",
    "train['Age'] = train[['Age', 'Pclass']].apply(impute_age, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Create function to replace the few empty Embark Values\n",
    "def impute_embark(cols, value):\n",
    "    embark = cols[0]\n",
    "    \n",
    "    if pd.isnull(embark):\n",
    "        return value\n",
    "    else: \n",
    "        return embark\n",
    "\n",
    "    \n",
    "train['Embarked'] = train[['Embarked']].apply(impute_embark, axis=1, args = (train['Embarked'].mode()[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "**It is always important when you modify data you check that it worked**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "sns.heatmap(train.isnull(), yticklabels=False, cbar=False, cmap='viridis')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "**There are barely any cabin entries we should ignore them**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# use .dropna() method to remove rows that contain missing values\n",
    "train.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Categorical Variables\n",
    "\n",
    "**What should we do with categorical variables?**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "We need to convert categorical variables (not understood by computers) into dummy variables which can be understood.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Produces what is known as a one-hot encoded vector\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "![](./figs/one_hot_encoded.png)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Use the .get_dummies() method to convert categorical data into dummy values\n",
    "# train['Sex'] refers to the column we want to convert\n",
    "# drop_first -> argument avoids the multicollinearity problem, which can undermines\n",
    "# the statistical significance of an independent variable.\n",
    "sex = pd.get_dummies(train['Sex'], drop_first=True) # drop first means k-1 values\n",
    "embark = pd.get_dummies(train['Embarked'], drop_first=True)\n",
    "\n",
    "# Use  .concat() method to merge the series data into one dataframe\n",
    "train = pd.concat([train, sex, embark], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Drop any insignificant features from the dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Drop columns with categorical data\n",
    "train.drop(['Sex','Embarked','Ticket','Name','PassengerId'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Training the model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "#### Step 1: Split the data into features and labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Split data into 'X' features and 'y' target label sets\n",
    "X = train[['Pclass', 'Age', 'SibSp', 'Parch', 'Fare', 'male', 'Q',\n",
    "       'S']]\n",
    "y = train['Survived']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Step 2: Split the data into test and training sets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Split data set into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Step 3: Create and Train the Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Create instance (i.e. object) of LogisticRegression\n",
    "logmodel = LogisticRegression()\n",
    "\n",
    "# Fit the model using the training data\n",
    "# X_train -> parameter supplies the data features\n",
    "# y_train -> parameter supplies the target labels\n",
    "logmodel.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "predictions = logmodel.predict(X_test)\n",
    "print(classification_report(y_test, predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "#### F1-Score\n",
    "\n",
    "It is calculated from the precision and recall of the test\n",
    "\n",
    "- **Precision** is the number of true positive results divided by the number of all positive results, including those not identified correctly\n",
    "  - known as positive predictive value\n",
    "\n",
    "$$ Precision = \\frac{True\\,Positives}{True\\,Positives + False\\,Positives} $$\n",
    "\n",
    "    - within everything that is predicted positive this is the percentage correct\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- **Recall** is the number of true positive results divided by the number of all samples that should have been identified as positive.\n",
    "  - known as sensitivity in diagnostic binary classification\n",
    "\n",
    "$$ Recall = \\frac{True\\,Positives}{True\\,Positives + False\\,Negatives} $$\n",
    "\n",
    "    - what is the fraction of actual positive results the model found\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "$$\n",
    "F_1 = \\frac{2}{\\mathrm{recall}^{-1} + \\mathrm{precision}^{-1}} = 2 \\frac{\\mathrm{precision} \\cdot \\mathrm{recall}}{\\mathrm{precision} + \\mathrm{recall}} = \\frac{2\\mathrm{tp}}{2\\mathrm{tp} + \\mathrm{fp} + \\mathrm{fn}}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Step 4: Visualizing Results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# computes the confusion matrix\n",
    "cm = confusion_matrix(y_test, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Plots the confusion matrix\n",
    "\n",
    "plt.clf()\n",
    "plt.imshow(cm, interpolation='nearest', cmap=plt.cm.viridis)\n",
    "classNames = ['Negative','Positive']\n",
    "plt.title('Titanic Confusion Matrix')\n",
    "plt.ylabel('True label')\n",
    "plt.xlabel('Predicted label')\n",
    "tick_marks = np.arange(len(classNames))\n",
    "plt.xticks(tick_marks, classNames, rotation=45)\n",
    "plt.yticks(tick_marks, classNames)\n",
    "s = [['TN','FP'], ['FN', 'TP']]\n",
    "for i in range(2):\n",
    "    for j in range(2):\n",
    "        plt.text(j,i, str(s[i][j])+\" = \"+str(cm[i][j]))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Multiclass classification\n",
    "\n",
    "If you have several classes to predict, an option often used is to fit one-versus-all classifiers and then use a voting heuristic for the final decision.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "\n",
    "# loads the dataset\n",
    "X, y = load_iris(return_X_y=True)\n",
    "\n",
    "# Instantiates the model and computes the logistic regression\n",
    "clf = LogisticRegression(random_state=0, solver='lbfgs',\n",
    "                         multi_class='multinomial').fit(X, y)\n",
    "\n",
    "# This predicts the classes\n",
    "clf.predict(X[:2, :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# predicting the probability\n",
    "clf.predict_proba(X[:2, :]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Shows the score for the model\n",
    "clf.score(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Support Vector Machines\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- A type of discriminate machine learning model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- They try and build a plane in space that separates examples that belong to different classes with the widest possible margin\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- New examples that are mapped onto that space get \"classified\" based on which side of the boundary they fall on\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Example: Random Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# creating datasets X containing n_samples \n",
    "# Y containing two classes \n",
    "X, Y = make_blobs(n_samples=500, centers=2, \n",
    "                  random_state=0, cluster_std=0.40) \n",
    "  \n",
    "# plotting scatters  \n",
    "plt.scatter(X[:, 0], X[:, 1], c=Y, s=50, cmap='RdBu'); \n",
    "plt.show()  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "In this case, it is very easy to classify the points with a linear boundary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# creating line space between -1 to 3.5  \n",
    "xfit = np.linspace(-1, 3.5) \n",
    "  \n",
    "# plotting scatter \n",
    "plt.scatter(X[:, 0], X[:, 1], c=Y, s=50, cmap='RdBu') \n",
    "  \n",
    "# plot a line between the different sets of data \n",
    "for m, b, d in [(1, 0.65, 0.33), (0.5, 1.6, 0.55), (-0.2, 2.9, 0.2)]: \n",
    "    yfit = m * xfit + b \n",
    "    plt.plot(xfit, yfit, '-k') \n",
    "    plt.fill_between(xfit, yfit - d, yfit + d, edgecolor='none',  \n",
    "    color='#AAAAAA', alpha=0.4) \n",
    "  \n",
    "plt.xlim(-1, 3.5); \n",
    "plt.show() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- SVMs try and find the widest possible perpendicular distance between the dividing vector and the points\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Example: Breast Cancer Data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Step 1: Load the Data\n",
    "\n",
    "We'll use the built-in breast cancer dataset from `Scikit-Learn`. We can get with the load function:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "cancer = load_breast_cancer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Step 2: Understanding the Data\n",
    "\n",
    "The data set is presented in a dictionary form:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "cancer.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "The `DESCR` contains a description of the information in the dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "print(cancer['DESCR'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# A list of the feature names\n",
    "cancer['feature_names']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Step 3: Building the Data Structure for Training\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "df_feat = pd.DataFrame(cancer['data'],columns=cancer['feature_names'])\n",
    "df_feat.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Views how the labels are structured\n",
    "cancer['target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "df_target = pd.DataFrame(cancer['target'],columns=['Cancer'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Step 4: Validate the Data Structure\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "df_feat.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "### Generally, at this stage, one would visualize that data using domain knowledge\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Step 5: Test-Train Split\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(df_feat, np.ravel(df_target), test_size=0.30, random_state=101)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Step 6: Train the Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "model = SVC(gamma='auto')\n",
    "model.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Step 7: Visualize the Results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "predictions = model.predict(X_test)\n",
    "cm = confusion_matrix(y_test,predictions)\n",
    "\n",
    "plt.clf()\n",
    "plt.imshow(cm, interpolation='nearest', cmap=plt.cm.viridis)\n",
    "classNames = ['Negative','Positive']\n",
    "plt.title('Cancer Confusion Matrix')\n",
    "plt.ylabel('True label')\n",
    "plt.xlabel('Predicted label')\n",
    "tick_marks = np.arange(len(classNames))\n",
    "plt.xticks(tick_marks, classNames, rotation=45)\n",
    "plt.yticks(tick_marks, classNames)\n",
    "s = [['TN','FP'], ['FN', 'TP']]\n",
    "for i in range(2):\n",
    "    for j in range(2):\n",
    "        plt.text(j,i, str(s[i][j])+\" = \"+str(cm[i][j]))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "print(classification_report(y_test,predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "**This is not good ... Our Machine Thinks Everyone has Cancer!!!!**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "**What did we do wrong?**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Step 4a: Scaling the Data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "When models fail:\n",
    "\n",
    "1. Check how the data was normalized\n",
    "2. Optimize the model parameters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "df_feat.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- The features have vastly different scales which makes them weighted unequally\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "##### Standard Scaler\n",
    "\n",
    "Removes the mean and scales to a unit variance\n",
    "\n",
    "$$ z = \\frac{x - \\mu}{\\sigma}$$\n",
    "\n",
    "- $\\mu$ is the mean\n",
    "- $\\sigma$ is the standard deviation\n",
    "\n",
    "**The result is all features will have a mean of 0 and a variance of 1**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# It is usually a good idea to scale the data for SVM training.\n",
    "# We are cheating a bit in this example in scaling all of the data,\n",
    "# instead of fitting the transformation on the training set and\n",
    "# just applying it on the test set.\n",
    "\n",
    "\n",
    "scaler = StandardScaler()\n",
    "df_feat = scaler.fit_transform(df_feat)\n",
    "df_feat = pd.DataFrame(df_feat,columns=cancer['feature_names'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "df_feat.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Step 5: Test-Train Split\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(df_feat, np.ravel(df_target), test_size=0.30, random_state=101)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Step 6: Train the Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "model = SVC(gamma='auto')\n",
    "model.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Step 7: Visualize the Results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "predictions = model.predict(X_test)\n",
    "cm = confusion_matrix(y_test,predictions)\n",
    "\n",
    "plt.clf()\n",
    "plt.imshow(cm, interpolation='nearest', cmap=plt.cm.viridis)\n",
    "classNames = ['Negative','Positive']\n",
    "plt.title('Cancer Confusion Matrix')\n",
    "plt.ylabel('True label')\n",
    "plt.xlabel('Predicted label')\n",
    "tick_marks = np.arange(len(classNames))\n",
    "plt.xticks(tick_marks, classNames, rotation=45)\n",
    "plt.yticks(tick_marks, classNames)\n",
    "s = [['TN','FP'], ['FN', 'TP']]\n",
    "for i in range(2):\n",
    "    for j in range(2):\n",
    "        plt.text(j,i, str(s[i][j])+\" = \"+str(cm[i][j]))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "**That looks way better**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Hyperparameter tuning by grid-search\n",
    "\n",
    "There are many parameters which need to be tuned when building machine learning model. \n",
    "* It is non-inutitive what these parameters should be"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "### Example: Adults Census Data\n",
    "\n",
    "This is a dataset that contains information from the 1994 Census. The goal is to predict if a person makes more than \\$50,000 per year based on their demographic information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn import set_config\n",
    "\n",
    "set_config(display=\"diagram\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Step 1: Load the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "adult_census = pd.read_csv(\"./data/adult-census.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Step 2: Visualize and Clean the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "We extract the column containing the target."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "target_name = \"class\"\n",
    "target = adult_census[target_name]\n",
    "target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "We drop from our data the target and the `\"education-num\"` column which\n",
    "duplicates the information from the `\"education\"` column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "data = adult_census.drop(columns=[target_name, \"education-num\"])\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Step 3: Test Train Split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Once the dataset is loaded, we split it into a training and testing sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "data_train, data_test, target_train, target_test = train_test_split(\n",
    "    data, target, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Step 4: Building a Pipeline\n",
    "\n",
    "Within Scikit-learn there are tools to build a pipeline to automate preprocessing and data analysis. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "We will define a pipeline. It will handle both numerical and categorical features.\n",
    "\n",
    "The first step is to select all the categorical columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "categorical_columns_selector = selector(dtype_include=object)\n",
    "categorical_columns = categorical_columns_selector(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Here we will use a tree-based model as a classifier\n",
    "(i.e. `HistGradientBoostingClassifier`). That means:\n",
    "\n",
    "* Numerical variables don't need scaling;\n",
    "* Categorical variables can be dealt with an `OrdinalEncoder` even if the \n",
    "  coding order is not meaningful;\n",
    "* For tree-based models, the `OrdinalEncoder` avoids having high-dimensional \n",
    "  representations.\n",
    "\n",
    "We now build our `OrdinalEncoder` by passing it the known categories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "categorical_preprocessor = OrdinalEncoder(handle_unknown=\"use_encoded_value\",\n",
    "                                          unknown_value=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "We then use a `ColumnTransformer` to select the categorical columns and apply the `OrdinalEncoder` to them.\n",
    "\n",
    "* The `ColumnTransformer` applies a the transformation to the columns of a Pandas dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "preprocessor = ColumnTransformer([\n",
    "    ('cat_preprocessor', categorical_preprocessor, categorical_columns)],\n",
    "    remainder='passthrough', sparse_threshold=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Finally, we use a tree-based classifier (i.e. histogram gradient-boosting) to\n",
    "predict whether or not a person earns more than 50 k$ a year."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "model = Pipeline([\n",
    "    (\"preprocessor\", preprocessor),\n",
    "    (\"classifier\",\n",
    "     HistGradientBoostingClassifier(random_state=42, max_leaf_nodes=4))])\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Step 5: Tuning the Model Using GridSearch\n",
    "\n",
    "* `GridSearchCV` is a scikit-learn class that implements a very similar logic with less repetitive code.\n",
    "\n",
    "Let's see how to use the `GridSearchCV` estimator for doing such search. Since the grid-search will be costly, we will only explore the combination learning-rate and the maximum number of nodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "param_grid = {\n",
    "    'classifier__learning_rate': (0.01, 0.1, 1, 10),\n",
    "    'classifier__max_leaf_nodes': (3, 10, 30)}\n",
    "model_grid_search = GridSearchCV(model, param_grid=param_grid,\n",
    "                                 n_jobs=2, cv=2)\n",
    "model_grid_search.fit(data_train, target_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Step 6: Validation of the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Finally, we will check the accuracy of our model using the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "accuracy = model_grid_search.score(data_test, target_test)\n",
    "print(\n",
    "    f\"The test accuracy score of the grid-searched pipeline is: \"\n",
    "    f\"{accuracy:.2f}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<div class=\"admonition warning alert alert-danger\">\n",
    "<p class=\"first admonition-title\" style=\"font-weight: bold;\">Warning</p>\n",
    "<p>Be aware that the evaluation should normally be performed through\n",
    "cross-validation by providing <tt class=\"docutils literal\">model_grid_search</tt> as a model to the\n",
    "<tt class=\"docutils literal\">cross_validate</tt> function.</p>\n",
    "<p class=\"last\">Here, we used a single train-test split to to evaluate <tt class=\"docutils literal\">model_grid_search</tt>.\n",
    "If interested you can look into more detail about nested cross-validation,\n",
    "when you use cross-validation both for hyperparameter tuning and model\n",
    "evaluation.</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "The `GridSearchCV` estimator takes a `param_grid` parameter which defines\n",
    "all hyperparameters and their associated values. The grid-search will be in\n",
    "charge of creating all possible combinations and test them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "The number of combinations will be equal to the product of the\n",
    "number of values to explore for each parameter (e.g. in our example 4 x 3\n",
    "combinations). Thus, adding new parameters with their associated values to be\n",
    "explored become rapidly computationally expensive."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Once the grid-search is fitted, it can be used as any other predictor by\n",
    "calling `predict` and `predict_proba`. Internally, it will use the model with\n",
    "the best parameters found during `fit`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Get predictions for the 5 first samples using the estimator with the best\n",
    "parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "model_grid_search.predict(data_test.iloc[0:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "You can know about these parameters by looking at the `best_params_`\n",
    "attribute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "print(f\"The best set of parameters is: \"\n",
    "      f\"{model_grid_search.best_params_}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "In addition, we can inspect all results which are stored in the attribute\n",
    "`cv_results_` of the grid-search. We will filter some specific columns\n",
    "from these results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "cv_results = pd.DataFrame(model_grid_search.cv_results_).sort_values(\n",
    "    \"mean_test_score\", ascending=False)\n",
    "cv_results.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Let us focus on the most interesting columns and shorten the parameter\n",
    "names to remove the `\"param_classifier__\"` prefix for readability:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# get the parameter names\n",
    "column_results = [f\"param_{name}\" for name in param_grid.keys()]\n",
    "column_results += [\n",
    "    \"mean_test_score\", \"std_test_score\", \"rank_test_score\"]\n",
    "cv_results = cv_results[column_results]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "def shorten_param(param_name):\n",
    "    if \"__\" in param_name:\n",
    "        return param_name.rsplit(\"__\", 1)[1]\n",
    "    return param_name\n",
    "\n",
    "\n",
    "cv_results = cv_results.rename(shorten_param, axis=1)\n",
    "cv_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Visualizing Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "With only 2 parameters, we might want to visualize the grid-search as a\n",
    "heatmap. We need to transform our `cv_results` into a dataframe where:\n",
    "\n",
    "- the rows will correspond to the learning-rate values;\n",
    "- the columns will correspond to the maximum number of leaf;\n",
    "- the content of the dataframe will be the mean test scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "pivoted_cv_results = cv_results.pivot_table(\n",
    "    values=\"mean_test_score\", index=[\"learning_rate\"],\n",
    "    columns=[\"max_leaf_nodes\"])\n",
    "\n",
    "pivoted_cv_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "We can use a heatmap representation to show the above dataframe visually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "ax = sns.heatmap(pivoted_cv_results, annot=True, cmap=\"YlGnBu\", vmin=0.7,\n",
    "                 vmax=0.9)\n",
    "ax.invert_yaxis()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "The above tables highlights the following things:\n",
    "\n",
    "* for too high values of `learning_rate`, the generalization performance of the\n",
    "  model is degraded and adjusting the value of `max_leaf_nodes` cannot fix\n",
    "  that problem;\n",
    "* outside of this pathological region, we observe that the optimal choice\n",
    "  of `max_leaf_nodes` depends on the value of `learning_rate`;\n",
    "* in particular, we observe a \"diagonal\" of good models with an accuracy\n",
    "  close to the maximal of 0.87: when the value of `max_leaf_nodes` is\n",
    "  increased, one should decrease the value of `learning_rate` accordingly\n",
    "  to preserve a good accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "For now we will note that, in general, **there is no unique optimal parameter\n",
    "setting**: 4 models out of the 12 parameter configurations reach the maximal\n",
    "accuracy (up to small random fluctuations caused by the sampling of the\n",
    "training set)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "## Nonlinear Data\n",
    "\n",
    "So far, we have made an assumption that all data is linear, this is usally not the case. \n",
    "\n",
    "* We should add non-linearity to help learn nonlinear trends. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "from IPython.display import HTML\n",
    "HTML('<iframe width=\"868\" height=\"651\" src=\"https://www.youtube.com/embed/3liCbRZPrZA\" frameborder=\"0\" allow=\"accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen></iframe>')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "### Kernel Trick\n",
    "\n",
    "* A Kernel trick converts data to a higher dimensional space where classes are linearly separable\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Kernel methods owe their name to the use of kernel functions, which enable them to operate in a high-dimensional, implicit feature space without ever computing the coordinates of the data in that space, but rather by simply computing the inner products between the images of all pairs of data in the feature space.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Looking at Kernals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "def plot_svc_decision_function(model, ax=None, plot_support=True):\n",
    "    \"\"\"Plot the decision function for a 2D SVC\"\"\"\n",
    "    if ax is None:\n",
    "        ax = plt.gca()\n",
    "    xlim = ax.get_xlim()\n",
    "    ylim = ax.get_ylim()\n",
    "    \n",
    "    # create grid to evaluate model\n",
    "    x = np.linspace(xlim[0], xlim[1], 30)\n",
    "    y = np.linspace(ylim[0], ylim[1], 30)\n",
    "    Y, X = np.meshgrid(y, x)\n",
    "    xy = np.vstack([X.ravel(), Y.ravel()]).T\n",
    "    P = model.decision_function(xy).reshape(X.shape)\n",
    "    \n",
    "    # plot decision boundary and margins\n",
    "    ax.contour(X, Y, P, colors='k',\n",
    "               levels=[-1, 0, 1], alpha=0.5,\n",
    "               linestyles=['--', '-', '--'])\n",
    "    \n",
    "    # plot support vectors\n",
    "    if plot_support:\n",
    "        ax.scatter(model.support_vectors_[:, 0],\n",
    "                   model.support_vectors_[:, 1],\n",
    "                   s=300, linewidth=1, facecolors='none');\n",
    "    ax.set_xlim(xlim)\n",
    "    ax.set_ylim(ylim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "X, y = make_circles(100, factor=.1, noise=.1)\n",
    "\n",
    "clf = SVC(kernel='linear').fit(X, y)\n",
    "\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y, s=50, cmap='RdBu')\n",
    "plot_svc_decision_function(clf, plot_support=False);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* Clearly we cannot use a linear classifier to classify this data!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "We could apply a radial basis function centered on the central clump to expand this data into a higher dimensional and make it amenable to a linear classifier\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "from ipywidgets import interact, fixed\n",
    "\n",
    "\n",
    "from mpl_toolkits import mplot3d\n",
    "\n",
    "r = np.exp(-(X ** 2).sum(1))\n",
    "plt.figure(figsize=(8,8))\n",
    "ax = plt.axes(projection='3d')\n",
    "ax.scatter3D(X[:,0], X[:,1], r, c=r, cmap='RdBu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* Now, making a linear classifier is trivial\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Kernel Tricks\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* It is not possible to randomly try to find the correct location for a kernel when you have a large number of dimensions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* You can use a kernel trick - we do not have time to go into the mathamatics of kernel tricks. If you are interested in learning more go [here](https://en.wikipedia.org/wiki/Kernel_method)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Example: Using Kernel Tricks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "clf = SVC(kernel='rbf', C=1E6)\n",
    "clf.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "#### Plotting the results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "plt.scatter(X[:, 0], X[:, 1], c=y, s=50, cmap='RdBu')\n",
    "plot_svc_decision_function(clf)\n",
    "plt.scatter(clf.support_vectors_[:, 0], clf.support_vectors_[:, 1],\n",
    "            s=300, lw=1, facecolors='none');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* There are a bunch of kernels which you can use"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "#### Example: Visualization of Different Kernels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Code source: Gaël Varoquaux\n",
    "# License: BSD 3 clause\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import svm\n",
    "\n",
    "\n",
    "# Our dataset and targets\n",
    "X = np.c_[(.4, -.7),\n",
    "          (-1.5, -1),\n",
    "          (-1.4, -.9),\n",
    "          (-1.3, -1.2),\n",
    "          (-1.1, -.2),\n",
    "          (-1.2, -.4),\n",
    "          (-.5, 1.2),\n",
    "          (-1.5, 2.1),\n",
    "          (1, 1),\n",
    "          # --\n",
    "          (1.3, .8),\n",
    "          (1.2, .5),\n",
    "          (.2, -2),\n",
    "          (.5, -2.4),\n",
    "          (.2, -2.3),\n",
    "          (0, -2.7),\n",
    "          (1.3, 2.1)].T\n",
    "Y = [0] * 8 + [1] * 8\n",
    "\n",
    "# figure number\n",
    "fignum = 1\n",
    "\n",
    "# fit the model\n",
    "for kernel in ('linear', 'poly', 'rbf'):\n",
    "    clf = svm.SVC(kernel=kernel, gamma=2)\n",
    "    clf.fit(X, Y)\n",
    "\n",
    "    # plot the line, the points, and the nearest vectors to the plane\n",
    "    plt.figure(fignum, figsize=(4, 3))\n",
    "    plt.clf()\n",
    "\n",
    "    plt.scatter(clf.support_vectors_[:, 0], clf.support_vectors_[:, 1], s=80,\n",
    "                facecolors='none', zorder=10, edgecolors='k')\n",
    "    plt.scatter(X[:, 0], X[:, 1], c=Y, zorder=10, cmap=plt.cm.Paired,\n",
    "                edgecolors='k')\n",
    "\n",
    "    plt.axis('tight')\n",
    "    plt.title(kernel)\n",
    "    x_min = -3\n",
    "    x_max = 3\n",
    "    y_min = -3\n",
    "    y_max = 3\n",
    "\n",
    "    XX, YY = np.mgrid[x_min:x_max:200j, y_min:y_max:200j]\n",
    "    Z = clf.decision_function(np.c_[XX.ravel(), YY.ravel()])\n",
    "\n",
    "    # Put the result into a color plot\n",
    "    Z = Z.reshape(XX.shape)\n",
    "    plt.figure(fignum, figsize=(4, 3))\n",
    "    plt.pcolormesh(XX, YY, Z > 0, cmap=plt.cm.Paired)\n",
    "    plt.contour(XX, YY, Z, colors=['k', 'k', 'k'], linestyles=['--', '-', '--'],\n",
    "                levels=[-.5, 0, .5])\n",
    "\n",
    "    plt.xlim(x_min, x_max)\n",
    "    plt.ylim(y_min, y_max)\n",
    "\n",
    "    plt.xticks(())\n",
    "    plt.yticks(())\n",
    "    fignum = fignum + 1\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* It is rare, particularly with high-dimensional data that you will know what your kernel should be $\\rightarrow$ you can use hyperparameter tuning to discover the best model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Decision Trees\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "A thought experiment: When you wake up and decide to attend class there are many things that factor into your decision.\n",
    "\n",
    "1. The grade you want to get\n",
    "2. Did you get a good night sleep\n",
    "3. Do you have a lot of other work to do\n",
    "4. Do you think the class is important\n",
    "5. Are you feeling sick\n",
    "   ...\n",
    "\n",
    "<br> \n",
    "Each of these effect the outcome of if you show up to class or not ... You can try how much each of these features effect decision making\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* Let's assume your decision is highly based on the weather\n",
    "\n",
    "![](figs/Decision_Tree_1.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "![](figs/Decision_Tree_Terminology.png)\n",
    "\n",
    "- Nodes: split for a value of a particular attribute\n",
    "- Edges: results of a particular decision\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Root: node that performs the first split\n",
    "- Leaves: the terminal output of each split\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**How do we choose how to split the data?**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* We want to choose the splits that maximizes the amount of information gained on the split\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "**Entropy** : If a sample is all of one class, entropy = 0, if it is evenly divided entropy = 1.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "We need to compute two entropy for a attribute\n",
    "\n",
    "1. Entropy for a single class\n",
    "   $$E(T) = \\sum_{i=1}^c -p_i\\log_{2}{p_i}$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "2. The entropy for two attributes:\n",
    "   $$ E(T,X) = \\sum_{c\\in X} P(c)E(c)$$\n",
    "\n",
    "**_n.b_**: $\\in$ means part of\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "**Information Gained** : The reduction in the amount of entropy after conducting a particular split\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "1. Calculate the entropy of the target ... Did you go to class?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "2. The dataset is split on different attributes and the entropy for each branch is calculated. (equation for entropy of two attributes)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "3. Information gained is calculted as:\n",
    "   $$ Information \\ Gained = E(T) - E(T,X) $$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "4. Select the split with the largest information gained\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "5. Repeat this process until you reach a state with an entropy of 0, this is a leaf node!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* As you could imagine this process is computationally intensive $\\rightarrow$ You can improve the performance using a random forest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Random Forest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* Random forests rely on decision trees to reduce computational complexity relying on bagging algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* when training a tree, the search only on a subset of the original features taken at random. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* The goal is to inject additional randomization into the learning procedure to try to decorrelate the prediction errors of the individual trees."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Therefore, random forests are using **randomization on both axes of the data matrix**:\n",
    "\n",
    "- by **bootstrapping samples** for **each tree** in the forest;\n",
    "- randomly selecting a **subset of features** at **each node** of the tree."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Example: Random Forest\n",
    "\n",
    "We will illustrate the usage of a random forest classifier on the adult census dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "adult_census = pd.read_csv(\"./data/adult-census.csv\")\n",
    "target_name = \"class\"\n",
    "data = adult_census.drop(columns=[target_name, \"education-num\"])\n",
    "target = adult_census[target_name]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "We will encode this data as we did before"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "from sklearn.compose import make_column_transformer, make_column_selector\n",
    "\n",
    "categorical_encoder = OrdinalEncoder(\n",
    "    handle_unknown=\"use_encoded_value\", unknown_value=-1\n",
    ")\n",
    "preprocessor = make_column_transformer(\n",
    "    (categorical_encoder, make_column_selector(dtype_include=object)),\n",
    "    remainder=\"passthrough\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "\n",
    "We will first give a simple example where we will train a single decision tree classifier and check its generalization performance via cross-validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "tree = make_pipeline(preprocessor, DecisionTreeClassifier(random_state=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "scores_tree = cross_val_score(tree, data, target)\n",
    "\n",
    "print(f\"Decision tree classifier: \"\n",
    "      f\"{scores_tree.mean():.3f} ± {scores_tree.std():.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Bagging Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "We construct a `BaggingClassifier` with a decision tree classifier as base model. \n",
    "\n",
    "* A Bagging classifier is an ensemble meta-estimator that fits base classifiers each on random subsets of the original dataset and then aggregate their individual predictions (either by voting or by averaging) to form a final prediction. \n",
    "* Such a meta-estimator can typically be used as a way to reduce the variance of a black-box estimator (e.g., a decision tree), by introducing randomization into its construction procedure and then making an ensemble out of it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import BaggingClassifier\n",
    "\n",
    "bagged_trees = make_pipeline(\n",
    "    preprocessor,\n",
    "    BaggingClassifier(\n",
    "        estimator=DecisionTreeClassifier(random_state=0),\n",
    "        n_estimators=50,\n",
    "        n_jobs=2,\n",
    "        random_state=0,\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "scores_bagged_trees = cross_val_score(bagged_trees, data, target)\n",
    "\n",
    "print(f\"Bagged decision tree classifier: \"\n",
    "      f\"{scores_bagged_trees.mean():.3f} ± {scores_bagged_trees.std():.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Note that the generalization performance of the bagged trees is already much\n",
    "better than the performance of a single tree."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Random Forest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Now, we will use a random forest. You will observe that we do not need to\n",
    "specify any `base_estimator` because the estimator is forced to be a decision\n",
    "tree. Thus, we just specify the desired number of trees in the forest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "random_forest = make_pipeline(\n",
    "    preprocessor,\n",
    "    RandomForestClassifier(n_estimators=50, n_jobs=2, random_state=0)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "scores_random_forest = cross_val_score(random_forest, data, target)\n",
    "\n",
    "print(f\"Random forest classifier: \"\n",
    "      f\"{scores_random_forest.mean():.3f} ± \"\n",
    "      f\"{scores_random_forest.std():.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "It seems that the random forest is performing slightly better than the bagged\n",
    "trees possibly due to the randomized selection of the features which\n",
    "decorrelates the prediction errors of individual trees and as a consequence\n",
    "make the averaging step more efficient at reducing overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "##### Details about default hyperparameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "For random forests, it is possible to control the amount of randomness for\n",
    "each split by setting the value of `max_features` hyperparameter:\n",
    "\n",
    "- `max_features=0.5` means that 50% of the features are considered at each\n",
    "  split;\n",
    "- `max_features=1.0` means that all features are considered at each split\n",
    "  which effectively disables feature subsampling."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "By default, `RandomForestRegressor` disables feature subsampling while\n",
    "`RandomForestClassifier` uses `max_features=np.sqrt(n_features)`. These\n",
    "default values reflect good practices given in the scientific literature."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "However, `max_features` is one of the hyperparameters to consider when tuning\n",
    "a random forest:\n",
    "- too much randomness in the trees can lead to underfitted base models and\n",
    "  can be detrimental for the ensemble as a whole,\n",
    "- too few randomness in the trees leads to more correlation of the prediction\n",
    "  errors and as a result reduce the benefits of the averaging step in terms\n",
    "  of overfitting control."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "In scikit-learn, the bagging classes also expose a `max_features` parameter.\n",
    "However, `BaggingClassifier` and `BaggingRegressor` are agnostic with respect\n",
    "to their base model and therefore random feature subsampling can only happen\n",
    "once before fitting each base model instead of several times per base model\n",
    "as is the case when adding splits to a given tree.\n",
    "\n",
    "We summarize these details in the following table:\n",
    "\n",
    "| Ensemble model class     | Base model class          | Default value for `max_features`   | Features subsampling strategy |\n",
    "|--------------------------|---------------------------|------------------------------------|-------------------------------|\n",
    "| `BaggingClassifier`      | User specified (flexible) | `n_features` (no&nbsp;subsampling) | Model level                   |\n",
    "| `RandomForestClassifier` | `DecisionTreeClassifier`  | `sqrt(n_features)`                 | Tree node level               |\n",
    "| `BaggingRegressor`       | User specified (flexible) | `n_features` (no&nbsp;subsampling) | Model level                   |\n",
    "| `RandomForestRegressor`  | `DecisionTreeRegressor`   | `n_features` (no&nbsp;subsampling) | Tree node level               |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Random Forest Regression\n",
    "\n",
    "* You can also use random forests for regression problems"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Let's start by generating some non-linear data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a random number generator that will be used to set the randomness\n",
    "rng = np.random.RandomState(0)\n",
    "\n",
    "\n",
    "def generate_data(n_samples=50):\n",
    "    \"\"\"Generate synthetic dataset. Returns `data_train`, `data_test`,\n",
    "    `target_train`.\"\"\"\n",
    "    x_max, x_min = 1.4, -1.4\n",
    "    len_x = x_max - x_min\n",
    "    x = rng.rand(n_samples) * len_x - len_x / 2\n",
    "    noise = rng.randn(n_samples) * 0.3\n",
    "    y = x ** 3 - 0.5 * x ** 2 + noise\n",
    "\n",
    "    data_train = pd.DataFrame(x, columns=[\"Feature\"])\n",
    "    data_test = pd.DataFrame(np.linspace(x_max, x_min, num=300),\n",
    "                             columns=[\"Feature\"])\n",
    "    target_train = pd.Series(y, name=\"Target\")\n",
    "\n",
    "    return data_train, data_test, target_train\n",
    "\n",
    "\n",
    "data_train, data_test, target_train = generate_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "sns.scatterplot(x=data_train[\"Feature\"], y=target_train, color=\"black\",\n",
    "                alpha=0.5)\n",
    "_ = plt.title(\"Synthetic regression dataset\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "We will start by creating a decision tree regressor. We will set\n",
    "the depth of the tree so that the resulting learner will underfit the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "tree = DecisionTreeRegressor(max_depth=3, random_state=0)\n",
    "tree.fit(data_train, target_train)\n",
    "\n",
    "target_train_predicted = tree.predict(data_train)\n",
    "target_test_predicted = tree.predict(data_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Using the term \"test\" here refers to data that was not used for training.\n",
    "It should not be confused with data coming from a train-test split, as it\n",
    "was generated in equally-spaced intervals for the visual evaluation of the\n",
    "predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# plot the data\n",
    "sns.scatterplot(x=data_train[\"Feature\"], y=target_train, color=\"black\",\n",
    "                alpha=0.5)\n",
    "# plot the predictions\n",
    "line_predictions = plt.plot(data_test[\"Feature\"], target_test_predicted, \"--\")\n",
    "\n",
    "# plot the residuals\n",
    "for value, true, predicted in zip(data_train[\"Feature\"],\n",
    "                                  target_train,\n",
    "                                  target_train_predicted):\n",
    "    lines_residuals = plt.plot([value, value], [true, predicted], color=\"red\")\n",
    "\n",
    "plt.legend([line_predictions[0], lines_residuals[0]],\n",
    "           [\"Fitted tree\", \"Residuals\"])\n",
    "_ = plt.title(\"Prediction function together \\nwith errors on the training set\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Our initial tree was not expressive enough to handle the complexity\n",
    "of the data. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Gradient-boosting decision tree (GBDT)\n",
    "\n",
    "GBDT will fit a decision tree on the residuals error (hence the name \"gradient\") of the previous tree.\n",
    "Therefore, each new tree in the ensemble predicts the error made by the previous learner instead of predicting the target directly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "In a gradient-boosting algorithm, the idea is to create a second tree which, given the same data `data`, will try\n",
    "to predict the residuals instead of the vector `target`. We would therefore have a tree that is able to predict the errors made by the initial tree.\n",
    "\n",
    "Let's train such a tree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "residuals = target_train - target_train_predicted\n",
    "\n",
    "tree_residuals = DecisionTreeRegressor(max_depth=5, random_state=0)\n",
    "tree_residuals.fit(data_train, residuals)\n",
    "\n",
    "target_train_predicted_residuals = tree_residuals.predict(data_train)\n",
    "target_test_predicted_residuals = tree_residuals.predict(data_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "sns.scatterplot(x=data_train[\"Feature\"], y=residuals, color=\"black\", alpha=0.5)\n",
    "line_predictions = plt.plot(\n",
    "    data_test[\"Feature\"], target_test_predicted_residuals, \"--\")\n",
    "\n",
    "# plot the residuals of the predicted residuals\n",
    "for value, true, predicted in zip(data_train[\"Feature\"],\n",
    "                                  residuals,\n",
    "                                  target_train_predicted_residuals):\n",
    "    lines_residuals = plt.plot([value, value], [true, predicted], color=\"red\")\n",
    "\n",
    "plt.legend([line_predictions[0], lines_residuals[0]],\n",
    "           [\"Fitted tree\", \"Residuals\"], bbox_to_anchor=(1.05, 0.8),\n",
    "           loc=\"upper left\")\n",
    "_ = plt.title(\"Prediction of the previous residuals\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "We see that this new tree only manages to fit some of the residuals. \n",
    "* We will focus on a specific sample from the training set (i.e. we know that the sample will be well predicted using two successive trees). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Let's first select this sample in `data_train`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "sample = data_train.iloc[[-2]]\n",
    "x_sample = sample['Feature'].iloc[0]\n",
    "target_true = target_train.iloc[-2]\n",
    "target_true_residual = residuals.iloc[-2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Let's plot the previous information and highlight our sample of interest.\n",
    "Let's start by plotting the original data and the prediction of the first\n",
    "decision tree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Plot the previous information:\n",
    "#   * the dataset\n",
    "#   * the predictions\n",
    "#   * the residuals\n",
    "\n",
    "sns.scatterplot(x=data_train[\"Feature\"], y=target_train, color=\"black\",\n",
    "                alpha=0.5)\n",
    "plt.plot(data_test[\"Feature\"], target_test_predicted, \"--\")\n",
    "for value, true, predicted in zip(data_train[\"Feature\"],\n",
    "                                  target_train,\n",
    "                                  target_train_predicted):\n",
    "    lines_residuals = plt.plot([value, value], [true, predicted], color=\"red\")\n",
    "\n",
    "# Highlight the sample of interest\n",
    "plt.scatter(sample, target_true, label=\"Sample of interest\",\n",
    "            color=\"tab:orange\", s=200)\n",
    "plt.xlim([-1, 0])\n",
    "plt.legend(bbox_to_anchor=(1.05, 0.8), loc=\"upper left\")\n",
    "_ = plt.title(\"Tree predictions\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Now, let's plot the residuals information. We will plot the residuals\n",
    "computed from the first decision tree and show the residual predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the previous information:\n",
    "#   * the residuals committed by the first tree\n",
    "#   * the residual predictions\n",
    "#   * the residuals of the residual predictions\n",
    "\n",
    "sns.scatterplot(x=data_train[\"Feature\"], y=residuals,\n",
    "                color=\"black\", alpha=0.5)\n",
    "plt.plot(data_test[\"Feature\"], target_test_predicted_residuals, \"--\")\n",
    "for value, true, predicted in zip(data_train[\"Feature\"],\n",
    "                                  residuals,\n",
    "                                  target_train_predicted_residuals):\n",
    "    lines_residuals = plt.plot([value, value], [true, predicted], color=\"red\")\n",
    "\n",
    "# Highlight the sample of interest\n",
    "plt.scatter(sample, target_true_residual, label=\"Sample of interest\",\n",
    "            color=\"tab:orange\", s=200)\n",
    "plt.xlim([-1, 0])\n",
    "plt.legend()\n",
    "_ = plt.title(\"Prediction of the residuals\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "For our sample of interest, our initial tree is making an error (small residual). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "When fitting the second tree, the residual in this case is\n",
    "perfectly fitted and predicted. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "print(f\"True value to predict for \"\n",
    "      f\"f(x={x_sample:.3f}) = {target_true:.3f}\")\n",
    "\n",
    "y_pred_first_tree = tree.predict(sample)[0]\n",
    "print(f\"Prediction of the first decision tree for x={x_sample:.3f}: \"\n",
    "      f\"y={y_pred_first_tree:.3f}\")\n",
    "print(f\"Error of the tree: {target_true - y_pred_first_tree:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "As we visually observed, we have a small error. Now, we can use the second\n",
    "tree to try to predict this residual."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "print(f\"Prediction of the residual for x={x_sample:.3f}: \"\n",
    "      f\"{tree_residuals.predict(sample)[0]:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "We see that our second tree is capable of predicting the exact residual\n",
    "(error) of our first tree. Therefore, we can predict the value of `x` by\n",
    "summing the prediction of all the trees in the ensemble."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "y_pred_first_and_second_tree = (\n",
    "    y_pred_first_tree + tree_residuals.predict(sample)[0]\n",
    ")\n",
    "print(f\"Prediction of the first and second decision trees combined for \"\n",
    "      f\"x={x_sample:.3f}: y={y_pred_first_and_second_tree:.3f}\")\n",
    "print(f\"Error of the tree: {target_true - y_pred_first_and_second_tree:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "We chose a sample for which only two trees were enough to make the perfect\n",
    "prediction. \n",
    "* One needs to add several trees to the ensemble to successfully correct the error (i.e. the second tree corrects the first tree's error, while the third tree corrects the second tree's error and so on)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Example: Gradient-Boosted Random Forest Regression\n",
    "\n",
    "We will compare the generalization performance of random-forest and gradient boosting on the California housing dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "##### Step 1: Loads the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.model_selection import cross_validate\n",
    "\n",
    "data, target = fetch_california_housing(return_X_y=True, as_frame=True)\n",
    "target *= 100  # rescale the target in k$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "##### Step 2: Builds the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "\n",
    "gradient_boosting = GradientBoostingRegressor(n_estimators=200)\n",
    "cv_results_gbdt = cross_validate(\n",
    "    gradient_boosting, data, target, scoring=\"neg_mean_absolute_error\",\n",
    "    n_jobs=10,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "##### Step 3: Views the Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "print(\"Gradient Boosting Decision Tree\")\n",
    "print(f\"Mean absolute error via cross-validation: \"\n",
    "      f\"{-cv_results_gbdt['test_score'].mean():.3f} ± \"\n",
    "      f\"{cv_results_gbdt['test_score'].std():.3f} k$\")\n",
    "print(f\"Average fit time: \"\n",
    "      f\"{cv_results_gbdt['fit_time'].mean():.3f} seconds\")\n",
    "print(f\"Average score time: \"\n",
    "      f\"{cv_results_gbdt['score_time'].mean():.3f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "##### Step 4: Compairison to Random Forest Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "random_forest = RandomForestRegressor(n_estimators=200, n_jobs=2)\n",
    "cv_results_rf = cross_validate(\n",
    "    random_forest, data, target, scoring=\"neg_mean_absolute_error\",\n",
    "    n_jobs=2,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "print(\"Random Forest\")\n",
    "print(f\"Mean absolute error via cross-validation: \"\n",
    "      f\"{-cv_results_rf['test_score'].mean():.3f} ± \"\n",
    "      f\"{cv_results_rf['test_score'].std():.3f} k$\")\n",
    "print(f\"Average fit time: \"\n",
    "      f\"{cv_results_rf['fit_time'].mean():.3f} seconds\")\n",
    "print(f\"Average score time: \"\n",
    "      f\"{cv_results_rf['score_time'].mean():.3f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "In term of computation performance, the forest can be parallelized and will\n",
    "benefit from using multiple cores of the CPU. In terms of scoring\n",
    "performance, both algorithms lead to very close results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "However, we see that the gradient boosting is a very fast algorithm to\n",
    "predict compared to random forest. This is due to the fact that gradient\n",
    "boosting uses shallow trees. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Model Selection and Parameter Optimization\n",
    "\n",
    "Looking at this new jersey diner menu of machine learning options with hyperparameters you might be wondering how do you select the best model\n",
    "\n",
    "* Generally speaking the computational cost of training is minimal, you can just use brute force\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Let's go back to the digits dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn import datasets, svm\n",
    "\n",
    "# Loads the digits dataset\n",
    "digits = datasets.load_digits()\n",
    "X_digits = digits.data\n",
    "y_digits = digits.target\n",
    "\n",
    "# instantiates and fits the support vector machine\n",
    "svc = svm.SVC(C=1, kernel='linear')\n",
    "svc.fit(X_digits[:-100], y_digits[:-100]).score(X_digits[-100:], y_digits[-100:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**To get a better sense you can run multiple trials splitting the data into different _folds_ for testing**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "X_folds = np.array_split(X_digits, 3)\n",
    "y_folds = np.array_split(y_digits, 3)\n",
    "scores = list()\n",
    "for k in range(3):\n",
    "    # We use 'list' to copy, in order to 'pop' later on\n",
    "    X_train = list(X_folds)\n",
    "    X_test = X_train.pop(k)\n",
    "    X_train = np.concatenate(X_train)\n",
    "    y_train = list(y_folds)\n",
    "    y_test = y_train.pop(k)\n",
    "    y_train = np.concatenate(y_train)\n",
    "    scores.append(svc.fit(X_train, y_train).score(X_test, y_test))\n",
    "print(scores)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "All the results are close, but they are slightly different."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**Sklearn has tools that automate cross-validation methods**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold, cross_val_score\n",
    "X = [\"a\", \"a\", \"a\", \"b\", \"b\", \"c\", \"c\", \"c\", \"c\", \"c\"]\n",
    "k_fold = KFold(n_splits=5)\n",
    "for train_indices, test_indices in k_fold.split(X):\n",
    "     print('Train: %s | test: %s' % (train_indices, test_indices))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "for train, test in k_fold.split(X_digits):  \n",
    "    print(svc.fit(X_digits[train], y_digits[train]).score(X_digits[test], y_digits[test]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**The cross-validation score can be directly calculated using the `cross_val_score helper`**\n",
    "  - Given an estimator, the `cross-validation` object and the input dataset, the `cross_val_score` splits the data repeatedly into a training and a testing set, trains the estimator using the training set and computes the scores based on the testing set for each iteration of cross-validation.\n",
    "\n",
    "**_n.b._** - `n_jobs=-1` allows for parallel processing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [],
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "scores = cross_val_score(svc, X_digits, y_digits, cv=k_fold,\n",
    "                scoring='precision_macro', n_jobs = -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    " print(\"Accuracy: %0.2f (+/- %0.2f)\" % (scores.mean(), scores.std() * 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**There are a ton of different methods to do cross-validation**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "IFrame('https://scikit-learn.org/stable/modules/cross_validation.html', width=1000, height=800)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**These can be passed as iterators into the cross-validation scoring estimator**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import ShuffleSplit, cross_val_score\n",
    " \n",
    "clf = svm.SVC(kernel='linear', C=1)\n",
    "cv = ShuffleSplit(n_splits=5, test_size=0.3, random_state=0)\n",
    "cross_val_score(clf, X_digits, y_digits, cv=cv)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**Cross validation metrics can be combined with GridSearch**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV, cross_val_score\n",
    "\n",
    "cv = ShuffleSplit(n_splits=5, test_size=0.3, random_state=0)\n",
    "\n",
    "Cs = np.logspace(-6, -1, 100)\n",
    "clf = GridSearchCV(estimator=svc, param_grid=dict(C=Cs),\n",
    "                   n_jobs=-1, cv=cv)\n",
    "clf.fit(X_digits[:1000], y_digits[:1000])        \n",
    "\n",
    "print(f'best score: {clf.best_score_ }')                                 \n",
    "\n",
    "print(f'best estimator: {clf.best_estimator_.C}')                                                            \n",
    "\n",
    "print('Prediction performance on test set is not as good as on train set')\n",
    "print(f'{clf.score(X_digits[1000:], y_digits[1000:])}')"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
