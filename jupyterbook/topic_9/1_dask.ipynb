{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Understanding Parellism in Python with Dask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Part 1: DASK Fundamentals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "### What is DASK?\n",
    "\n",
    "DASK is a flexible parallel computing library that:\n",
    "- Provides parallel DataFrame and Array abstractions\n",
    "- Builds task graphs dynamically\n",
    "- Executes these graphs in parallel\n",
    "- Scales from laptops to clusters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "### Installing Dask\n",
    "\n",
    "You can install Dask using pip:\n",
    "\n",
    "```bash\n",
    "pip install dask\n",
    "```\n",
    "\n",
    "or conda:\n",
    "\n",
    "```bash\n",
    "conda install dask\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "import dask\n",
    "import dask.dataframe as dd\n",
    "import dask.array as da\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from dask.distributed import Client, LocalCluster"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Understanding DASK's Architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "### 1. Task Graphs\n",
    "DASK represents computations as task graphs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Simple example of task graph creation\n",
    "@dask.delayed\n",
    "def add(a, b):\n",
    "    return a + b\n",
    "\n",
    "@dask.delayed\n",
    "def multiply(a, b):\n",
    "    return a * b\n",
    "\n",
    "# Create a simple computation\n",
    "x = add(1, 2)\n",
    "y = multiply(x, 3)\n",
    "\n",
    "# Visualize the task graph\n",
    "y.visualize()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "### 2. Schedulers\n",
    "DASK offers multiple schedulers:\n",
    "- Single-thread\n",
    "- Multi-thread\n",
    "- Multi-process\n",
    "- Distributed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Creating a local cluster\n",
    "cluster = LocalCluster(n_workers=10, threads_per_worker=2)\n",
    "client = Client(cluster)\n",
    "print(\"Cluster Dashboard:\", client.dashboard_link)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## DASK Collections"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "### 1. DASK Arrays\n",
    "Similar to NumPy arrays, but divided into chunks:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Create a large array\n",
    "x = da.random.random((10000, 10000), chunks=(1000, 1000))\n",
    "print(\"Array shape:\", x.shape)\n",
    "print(\"Chunk shape:\", x.chunks)\n",
    "\n",
    "# Show how chunks are processed\n",
    "result = x.mean(axis=0)\n",
    "result.visualize()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "### 2. DASK DataFrames\n",
    "Parallel implementation of Pandas:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Create a simple DataFrame with guaranteed equal lengths\n",
    "n_rows = 1000000\n",
    "n_repeats = (n_rows + 2) // 3  # Ceiling division to ensure we have enough elements\n",
    "df = pd.DataFrame({\n",
    "    'value': np.arange(n_rows),  # Using numpy array instead of range\n",
    "    'category': ((['A'] * n_repeats + ['B'] * n_repeats + ['C'] * n_repeats)[:n_rows])\n",
    "})\n",
    "\n",
    "# Convert to DASK DataFrame\n",
    "ddf = dd.from_pandas(df, npartitions=4)\n",
    "print(\"Number of partitions:\", ddf.npartitions)\n",
    "print(\"Partition sizes:\", ddf.map_partitions(len).compute())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Understanding Lazy Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Create a computation without executing it\n",
    "x = da.random.random((1000, 1000), chunks=(100, 100))\n",
    "y = x + x.T\n",
    "z = y.mean(axis=0)\n",
    "\n",
    "# Nothing computed yet\n",
    "print(\"Computation defined but not executed\")\n",
    "\n",
    "# Execute computation\n",
    "result = z.compute()\n",
    "print(\"Computation completed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Memory Management and Optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "## 1. Chunking Strategies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Bad chunking (too many small chunks)\n",
    "bad_chunks = da.random.random((10000, 10000), chunks=(100, 100))\n",
    "\n",
    "# Better chunking\n",
    "good_chunks = da.random.random((10000, 10000), chunks=(1000, 1000))\n",
    "\n",
    "# Compare task graphs\n",
    "print(\"Number of tasks (bad chunking):\", len(bad_chunks.dask))\n",
    "print(\"Number of tasks (good chunking):\", len(good_chunks.dask))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "## 2. Persist vs Compute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Persist keeps data in memory\n",
    "x = da.random.random((5000, 5000), chunks=(1000, 1000))\n",
    "x_persisted = x.persist()\n",
    "\n",
    "# Multiple computations are now faster\n",
    "%time result0 = (x+1).mean().compute()\n",
    "%time result1 = (x * 2).mean().compute()\n",
    "%time result2 = (x_persisted + 1).mean().compute()\n",
    "%time result3 = (x_persisted * 2).mean().compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Part 2: Engineering Applications"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "### Example 1: Processing Time Series Data\n",
    "\n",
    "Understanding how DASK handles large time series datasets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "def generate_sensor_data(n_points):\n",
    "    \"\"\"Generate sample sensor data\"\"\"\n",
    "    time = np.linspace(0, 100, n_points)\n",
    "    signal = np.sin(0.1 * time) + 0.1 * np.random.random(n_points)\n",
    "    return pd.DataFrame({\n",
    "        'time': time,\n",
    "        'signal': signal\n",
    "    })\n",
    "\n",
    "# Create large dataset\n",
    "df = generate_sensor_data(10000000)\n",
    "ddf = dd.from_pandas(df, npartitions=4)\n",
    "\n",
    "# Show how DASK partitions the data\n",
    "print(\"Partition info:\")\n",
    "print(ddf.divisions)  # Boundary values between partitions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "### Understanding Parallel Operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Define a complex operation\n",
    "@dask.delayed\n",
    "def process_chunk(data):\n",
    "    \"\"\"Process a chunk of data\"\"\"\n",
    "    # Simulate complex calculation\n",
    "    result = np.fft.fft(data['signal'])\n",
    "    return np.abs(result).mean()\n",
    "\n",
    "# Apply to partitions\n",
    "results = []\n",
    "for partition in range(ddf.npartitions):\n",
    "    chunk = ddf.get_partition(partition)\n",
    "    results.append(process_chunk(chunk))\n",
    "\n",
    "# Compute all results\n",
    "final_results = dask.compute(*results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "## Advanced DASK Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### 1. Custom Partitioning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Partition by time periods\n",
    "timestamps = pd.date_range('2024-01-01', '2024-12-31', freq='H')\n",
    "values = np.random.random(len(timestamps))\n",
    "\n",
    "df = pd.DataFrame({\n",
    "    'timestamp': timestamps,\n",
    "    'value': values\n",
    "})\n",
    "\n",
    "# Partition by month\n",
    "ddf = dd.from_pandas(df, npartitions=12)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### 2. Dashboard and Diagnostics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "from dask.distributed import progress\n",
    "\n",
    "def complex_calculation():\n",
    "    # Create large arrays\n",
    "    x = da.random.random((100000, 100000), chunks=(1000, 1000))\n",
    "    y = da.random.random((100000, 100000), chunks=(1000, 1000))\n",
    "    \n",
    "    # Multiple operations\n",
    "    z = (x + y).mean(axis=0)\n",
    "    w = (x - y).std(axis=1)\n",
    "    \n",
    "    # Show progress\n",
    "    progress(z, w)\n",
    "    \n",
    "    return dask.compute(z, w)\n",
    "\n",
    "results = complex_calculation()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Engineering Example: Vibration Analysis\n",
    "\n",
    "Now let's combine our DASK knowledge with engineering:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Generate vibration data\n",
    "def generate_vibration_data(n_samples, n_sensors):\n",
    "    time = np.linspace(0, 10, n_samples)\n",
    "    data = {}\n",
    "    \n",
    "    for sensor in range(n_sensors):\n",
    "        # Base frequency\n",
    "        f1 = 50 + np.random.random() * 10\n",
    "        # Harmonic\n",
    "        f2 = f1 * 2\n",
    "        \n",
    "        signal = (np.sin(2 * np.pi * f1 * time) + \n",
    "                 0.5 * np.sin(2 * np.pi * f2 * time) +\n",
    "                 0.1 * np.random.random(n_samples))\n",
    "        data[f'sensor_{sensor}'] = signal\n",
    "    \n",
    "    return pd.DataFrame(data, index=time)\n",
    "\n",
    "# Create large dataset\n",
    "df = generate_vibration_data(1000000, 16)\n",
    "ddf = dd.from_pandas(df, npartitions=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Define analysis function\n",
    "@dask.delayed\n",
    "def analyze_vibration(data):\n",
    "    \"\"\"Analyze vibration data chunk\"\"\"\n",
    "    # Calculate FFT\n",
    "    fft_result = np.fft.fft(data)\n",
    "    # Get magnitude\n",
    "    magnitude = np.abs(fft_result)\n",
    "    # Find peak frequencies\n",
    "    peak_freq = np.argmax(magnitude)\n",
    "    return peak_freq\n",
    "\n",
    "# Apply analysis to each sensor\n",
    "results = {}\n",
    "for column in ddf.columns:\n",
    "    results[column] = analyze_vibration(ddf[column])\n",
    "\n",
    "# Compute results\n",
    "peak_frequencies = dask.compute(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "peak_frequencies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Best Practices\n",
    "\n",
    "1. **Data Organization**\n",
    "   - Choose appropriate chunk sizes\n",
    "   - Consider data access patterns\n",
    "   - Use persist() for frequently accessed data\n",
    "\n",
    "2. **Performance Optimization**\n",
    "   - Monitor memory usage\n",
    "   - Use appropriate number of workers\n",
    "   - Balance parallelism and overhead\n",
    "\n",
    "3. **Debugging and Monitoring**\n",
    "   - Use the dashboard\n",
    "   - Monitor task progress\n",
    "   - Check worker logs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Real-world Considerations\n",
    "\n",
    "1. **When to Use DASK**\n",
    "   - Data doesn't fit in memory\n",
    "   - Computation is CPU-bound\n",
    "   - Parallel processing needed\n",
    "\n",
    "2. **When Not to Use DASK**\n",
    "   - Small datasets\n",
    "   - Simple computations\n",
    "   - I/O bound operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Cleanup\n",
    "client.close()\n",
    "cluster.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  },
  "rise": {
   "autolaunch": true,
   "enable_chalkboard": true,
   "theme": "simple",
   "transition": "slide"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
