{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Clustering Algorithms\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Attempts to find similar groupings within your dataset\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Imports Packages\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import warnings\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn import cluster, datasets\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cluster import KMeans, AgglomerativeClustering\n",
    "from sklearn.datasets import make_blobs\n",
    "from sklearn.metrics import pairwise_distances_argmin\n",
    "from sklearn.neighbors import kneighbors_graph\n",
    "\n",
    "from itertools import cycle, islice\n",
    "\n",
    "# Though the following import is not directly being used, it is required\n",
    "# for 3D projection to work\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "%matplotlib inline\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "## K-means clustering\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Tries to cluster data into n groups of equal variance by minimizing the _inertia_ or within-cluster sum-of-squares\n",
    "  $$\\sum_{i=0}^{n}\\min_{\\mu_{j} \\in C}(||x_i-\\mu_j||^2)$$\n",
    "  where $\\mu_j$ is the mean of the samples in the cluster\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- This is not a great metric and fails a lot ...\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# Author: Phil Roth <mr.phil.roth@gmail.com>\n",
    "# License: BSD 3 clause\n",
    "plt.figure(figsize=(12, 12))\n",
    "\n",
    "n_samples = 1500\n",
    "random_state = 170\n",
    "X, y = make_blobs(n_samples=n_samples, random_state=random_state)\n",
    "\n",
    "# Incorrect number of clusters\n",
    "y_pred = KMeans(n_clusters=2, random_state=random_state).fit_predict(X)\n",
    "\n",
    "plt.subplot(221)\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y_pred)\n",
    "plt.title(\"Incorrect Number of Blobs\")\n",
    "\n",
    "# Anisotropic distributed data\n",
    "transformation = [[0.60834549, -0.63667341], [-0.40887718, 0.85253229]]\n",
    "X_aniso = np.dot(X, transformation)\n",
    "y_pred = KMeans(n_clusters=3, random_state=random_state).fit_predict(X_aniso)\n",
    "\n",
    "plt.subplot(222)\n",
    "plt.scatter(X_aniso[:, 0], X_aniso[:, 1], c=y_pred)\n",
    "plt.title(\"Anisotropicly Distributed Blobs\")\n",
    "\n",
    "# Different variance\n",
    "X_varied, y_varied = make_blobs(\n",
    "    n_samples=n_samples, cluster_std=[1.0, 2.5, 0.5], random_state=random_state\n",
    ")\n",
    "y_pred = KMeans(n_clusters=3, random_state=random_state).fit_predict(X_varied)\n",
    "\n",
    "plt.subplot(223)\n",
    "plt.scatter(X_varied[:, 0], X_varied[:, 1], c=y_pred)\n",
    "plt.title(\"Unequal Variance\")\n",
    "\n",
    "# Unevenly sized blobs\n",
    "X_filtered = np.vstack((X[y == 0][:500], X[y == 1][:100], X[y == 2][:10]))\n",
    "y_pred = KMeans(n_clusters=3, random_state=random_state).fit_predict(X_filtered)\n",
    "\n",
    "plt.subplot(224)\n",
    "plt.scatter(X_filtered[:, 0], X_filtered[:, 1], c=y_pred)\n",
    "plt.title(\"Unevenly Sized Blobs\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## How Clustering works?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "1. Selects the initial points of the centroid\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Loops around two more steps\n",
    "\n",
    "1. Assigns each sample to its nearest centroid\n",
    "2. Computes the new centroid based on the mean\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "This process is repeated until it reaches some threshold value\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Visualizing K-means Clustering\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "X, y_true = make_blobs(n_samples=300, centers=4, cluster_std=0.60, random_state=0)\n",
    "\n",
    "rng = np.random.RandomState(42)\n",
    "centers = [0, 4] + rng.randn(4, 2)\n",
    "\n",
    "\n",
    "def draw_points(ax, c, factor=1):\n",
    "    ax.scatter(X[:, 0], X[:, 1], c=c, cmap=\"viridis\", s=50 * factor, alpha=0.3)\n",
    "\n",
    "\n",
    "def draw_centers(ax, centers, factor=1, alpha=1.0):\n",
    "    ax.scatter(\n",
    "        centers[:, 0],\n",
    "        centers[:, 1],\n",
    "        c=np.arange(4),\n",
    "        cmap=\"viridis\",\n",
    "        s=200 * factor,\n",
    "        alpha=alpha,\n",
    "    )\n",
    "    ax.scatter(centers[:, 0], centers[:, 1], c=\"black\", s=50 * factor, alpha=alpha)\n",
    "\n",
    "\n",
    "def make_ax(fig, gs):\n",
    "    ax = fig.add_subplot(gs)\n",
    "    ax.xaxis.set_major_formatter(plt.NullFormatter())\n",
    "    ax.yaxis.set_major_formatter(plt.NullFormatter())\n",
    "    return ax\n",
    "\n",
    "\n",
    "fig = plt.figure(figsize=(15, 4))\n",
    "gs = plt.GridSpec(\n",
    "    4, 15, left=0.02, right=0.98, bottom=0.05, top=0.95, wspace=0.2, hspace=0.2\n",
    ")\n",
    "ax0 = make_ax(fig, gs[:4, :4])\n",
    "ax0.text(\n",
    "    0.98,\n",
    "    0.98,\n",
    "    \"Random Initialization\",\n",
    "    transform=ax0.transAxes,\n",
    "    ha=\"right\",\n",
    "    va=\"top\",\n",
    "    size=16,\n",
    ")\n",
    "draw_points(ax0, \"gray\", factor=2)\n",
    "draw_centers(ax0, centers, factor=2)\n",
    "\n",
    "for i in range(3):\n",
    "    ax1 = make_ax(fig, gs[:2, 4 + 2 * i : 6 + 2 * i])\n",
    "    ax2 = make_ax(fig, gs[2:, 5 + 2 * i : 7 + 2 * i])\n",
    "\n",
    "    # E-step\n",
    "    y_pred = pairwise_distances_argmin(X, centers)\n",
    "    draw_points(ax1, y_pred)\n",
    "    draw_centers(ax1, centers)\n",
    "\n",
    "    # M-step\n",
    "    new_centers = np.array([X[y_pred == i].mean(0) for i in range(4)])\n",
    "    draw_points(ax2, y_pred)\n",
    "    draw_centers(ax2, centers, alpha=0.3)\n",
    "    draw_centers(ax2, new_centers)\n",
    "    for i in range(4):\n",
    "        ax2.annotate(\n",
    "            \"\",\n",
    "            new_centers[i],\n",
    "            centers[i],\n",
    "            arrowprops=dict(arrowstyle=\"->\", linewidth=1),\n",
    "        )\n",
    "\n",
    "    # Finish iteration\n",
    "    centers = new_centers\n",
    "    ax1.text(\n",
    "        0.95, 0.95, \"E-Step\", transform=ax1.transAxes, ha=\"right\", va=\"top\", size=14\n",
    "    )\n",
    "    ax2.text(\n",
    "        0.95, 0.95, \"M-Step\", transform=ax2.transAxes, ha=\"right\", va=\"top\", size=14\n",
    "    )\n",
    "\n",
    "\n",
    "# Final E-step\n",
    "y_pred = pairwise_distances_argmin(X, centers)\n",
    "axf = make_ax(fig, gs[:4, -4:])\n",
    "draw_points(axf, y_pred, factor=2)\n",
    "draw_centers(axf, centers, factor=2)\n",
    "axf.text(\n",
    "    0.98,\n",
    "    0.98,\n",
    "    \"Final Clustering\",\n",
    "    transform=axf.transAxes,\n",
    "    ha=\"right\",\n",
    "    va=\"top\",\n",
    "    size=16,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Clustering Example\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn import datasets\n",
    "\n",
    "# Set viewing angles\n",
    "elev = 48\n",
    "azim = 134\n",
    "\n",
    "# Set random seed\n",
    "np.random.seed(5)\n",
    "\n",
    "# Load Iris dataset\n",
    "iris = datasets.load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "# Define estimators for KMeans clustering\n",
    "estimators = [\n",
    "    (\"k_means_iris_8\", KMeans(n_clusters=8)),\n",
    "    (\"k_means_iris_3\", KMeans(n_clusters=3)),\n",
    "    (\"k_means_iris_bad_init\", KMeans(n_clusters=3, n_init=1, init=\"random\")),\n",
    "]\n",
    "\n",
    "# Plotting each clustering result\n",
    "fignum = 1\n",
    "titles = [\"8 clusters\", \"3 clusters\", \"3 clusters, bad initialization\"]\n",
    "for name, est in estimators:\n",
    "    fig = plt.figure(fignum, figsize=(4, 3))\n",
    "    ax = fig.add_subplot(111, projection=\"3d\", elev=elev, azim=azim)\n",
    "    est.fit(X)\n",
    "    labels = est.labels_\n",
    "\n",
    "    ax.scatter(X[:, 3], X[:, 0], X[:, 2], c=labels.astype(\"float\"), edgecolor=\"k\")\n",
    "\n",
    "    ax.set_xticklabels([])\n",
    "    ax.set_yticklabels([])\n",
    "    ax.set_zticklabels([])\n",
    "    ax.set_xlabel(\"Petal width\")\n",
    "    ax.set_ylabel(\"Sepal length\")\n",
    "    ax.set_zlabel(\"Petal length\")\n",
    "    ax.set_title(titles[fignum - 1])\n",
    "    ax.dist = 12\n",
    "    fignum += 1\n",
    "\n",
    "# Plot the ground truth\n",
    "fig = plt.figure(fignum, figsize=(4, 3))\n",
    "ax = fig.add_subplot(111, projection=\"3d\", elev=elev, azim=azim)\n",
    "\n",
    "for name, label in [(\"Setosa\", 0), (\"Versicolour\", 1), (\"Virginica\", 2)]:\n",
    "    ax.text3D(\n",
    "        X[y == label, 3].mean(),\n",
    "        X[y == label, 0].mean(),\n",
    "        X[y == label, 2].mean() + 2,\n",
    "        name,\n",
    "        horizontalalignment=\"center\",\n",
    "        bbox=dict(alpha=0.2, edgecolor=\"w\", facecolor=\"w\"),\n",
    "    )\n",
    "\n",
    "ax.scatter(X[:, 3], X[:, 0], X[:, 2], c=y, edgecolor=\"k\")\n",
    "\n",
    "ax.set_xticklabels([])\n",
    "ax.set_yticklabels([])\n",
    "ax.set_zticklabels([])\n",
    "ax.set_xlabel(\"Petal width\")\n",
    "ax.set_ylabel(\"Sepal length\")\n",
    "ax.set_zlabel(\"Petal length\")\n",
    "ax.set_title(\"Ground Truth\")\n",
    "ax.dist = 12\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Hierarchical Agglomerative Clustering\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Clustering algorithms that build clusters based on a hierarchy\n",
    "  - Agglomerative - Bottom-up approach where each observation starts as its cluster and then merges\n",
    "    - Fast when there are a large number of clusters\n",
    "  - Divisive - Top-down approach where all observations start as one cluster which is split iteratively.\n",
    "    - Slow when there are a large number of clusters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "\n",
    "n_samples = 1500\n",
    "noisy_circles = datasets.make_circles(n_samples=n_samples, factor=0.5, noise=0.05)\n",
    "noisy_moons = datasets.make_moons(n_samples=n_samples, noise=0.05)\n",
    "blobs = datasets.make_blobs(n_samples=n_samples, random_state=8)\n",
    "no_structure = np.random.rand(n_samples, 2), None\n",
    "\n",
    "# Anisotropicly distributed data\n",
    "random_state = 170\n",
    "X, y = datasets.make_blobs(n_samples=n_samples, random_state=random_state)\n",
    "transformation = [[0.6, -0.6], [-0.4, 0.8]]\n",
    "X_aniso = np.dot(X, transformation)\n",
    "aniso = (X_aniso, y)\n",
    "\n",
    "# blobs with varied variances\n",
    "varied = datasets.make_blobs(\n",
    "    n_samples=n_samples, cluster_std=[1.0, 2.5, 0.5], random_state=random_state\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Set up cluster parameters\n",
    "plt.figure(figsize=(9 * 1.3 + 2, 14.5))\n",
    "plt.subplots_adjust(\n",
    "    left=0.02, right=0.98, bottom=0.001, top=0.96, wspace=0.05, hspace=0.01\n",
    ")\n",
    "\n",
    "plot_num = 1\n",
    "\n",
    "default_base = {\"n_neighbors\": 10, \"n_clusters\": 3}\n",
    "\n",
    "datasets = [\n",
    "    (noisy_circles, {\"n_clusters\": 2}),\n",
    "    (noisy_moons, {\"n_clusters\": 2}),\n",
    "    (varied, {\"n_neighbors\": 2}),\n",
    "    (aniso, {\"n_neighbors\": 2}),\n",
    "    (blobs, {}),\n",
    "    (no_structure, {}),\n",
    "]\n",
    "\n",
    "for i_dataset, (dataset, algo_params) in enumerate(datasets):\n",
    "    # update parameters with dataset-specific values\n",
    "    params = default_base.copy()\n",
    "    params.update(algo_params)\n",
    "\n",
    "    X, y = dataset\n",
    "\n",
    "    # normalize dataset for easier parameter selection\n",
    "    X = StandardScaler().fit_transform(X)\n",
    "\n",
    "    # ============\n",
    "    # Create cluster objects\n",
    "    # ============\n",
    "    ward = cluster.AgglomerativeClustering(\n",
    "        n_clusters=params[\"n_clusters\"], linkage=\"ward\"\n",
    "    )\n",
    "    complete = cluster.AgglomerativeClustering(\n",
    "        n_clusters=params[\"n_clusters\"], linkage=\"complete\"\n",
    "    )\n",
    "    average = cluster.AgglomerativeClustering(\n",
    "        n_clusters=params[\"n_clusters\"], linkage=\"average\"\n",
    "    )\n",
    "    single = cluster.AgglomerativeClustering(\n",
    "        n_clusters=params[\"n_clusters\"], linkage=\"single\"\n",
    "    )\n",
    "\n",
    "    clustering_algorithms = (\n",
    "        (\"Single Linkage\", single),\n",
    "        (\"Average Linkage\", average),\n",
    "        (\"Complete Linkage\", complete),\n",
    "        (\"Ward Linkage\", ward),\n",
    "    )\n",
    "\n",
    "    for name, algorithm in clustering_algorithms:\n",
    "        t0 = time.time()\n",
    "\n",
    "        # catch warnings related to kneighbors_graph\n",
    "        with warnings.catch_warnings():\n",
    "            warnings.filterwarnings(\n",
    "                \"ignore\",\n",
    "                message=\"the number of connected components of the \"\n",
    "                + \"connectivity matrix is [0-9]{1,2}\"\n",
    "                + \" > 1. Completing it to avoid stopping the tree early.\",\n",
    "                category=UserWarning,\n",
    "            )\n",
    "            algorithm.fit(X)\n",
    "\n",
    "        t1 = time.time()\n",
    "        if hasattr(algorithm, \"labels_\"):\n",
    "            y_pred = algorithm.labels_.astype(\"int\")\n",
    "        else:\n",
    "            y_pred = algorithm.predict(X)\n",
    "\n",
    "        plt.subplot(len(datasets), len(clustering_algorithms), plot_num)\n",
    "        if i_dataset == 0:\n",
    "            plt.title(name, size=18)\n",
    "\n",
    "        colors = np.array(\n",
    "            list(\n",
    "                islice(\n",
    "                    cycle(\n",
    "                        [\n",
    "                            \"#377eb8\",\n",
    "                            \"#ff7f00\",\n",
    "                            \"#4daf4a\",\n",
    "                            \"#f781bf\",\n",
    "                            \"#a65628\",\n",
    "                            \"#984ea3\",\n",
    "                            \"#999999\",\n",
    "                            \"#e41a1c\",\n",
    "                            \"#dede00\",\n",
    "                        ]\n",
    "                    ),\n",
    "                    int(max(y_pred) + 1),\n",
    "                )\n",
    "            )\n",
    "        )\n",
    "        plt.scatter(X[:, 0], X[:, 1], s=10, color=colors[y_pred])\n",
    "\n",
    "        plt.xlim(-2.5, 2.5)\n",
    "        plt.ylim(-2.5, 2.5)\n",
    "        plt.xticks(())\n",
    "        plt.yticks(())\n",
    "        plt.text(\n",
    "            0.99,\n",
    "            0.01,\n",
    "            (\"%.2fs\" % (t1 - t0)).lstrip(\"0\"),\n",
    "            transform=plt.gca().transAxes,\n",
    "            size=15,\n",
    "            horizontalalignment=\"right\",\n",
    "        )\n",
    "        plot_num += 1\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "The main observations to make are:\n",
    "\n",
    "- single linkage is fast, and can perform well on non-globular data, but it performs poorly in the presence of noise.\n",
    "- average and complete linkage perform well on cleanly separated globular clusters, but have mixed results otherwise.\n",
    "- Ward is the most effective method for noisy data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Connectivity-Constrained Clustering\n",
    "\n",
    "- Single, average, and complete linkage are unstable and tend to make a few clusters that grow quickly\n",
    "- This means that only connected structures can be merged together\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Authors: Gael Varoquaux, Nelle Varoquaux\n",
    "# License: BSD 3 clause\n",
    "\n",
    "# Generate sample data\n",
    "n_samples = 1500\n",
    "np.random.seed(0)\n",
    "t = 1.5 * np.pi * (1 + 3 * np.random.rand(1, n_samples))\n",
    "x = t * np.cos(t)\n",
    "y = t * np.sin(t)\n",
    "\n",
    "\n",
    "X = np.concatenate((x, y))\n",
    "X += 0.7 * np.random.randn(2, n_samples)\n",
    "X = X.T\n",
    "\n",
    "# Create a graph capturing local connectivity. Larger number of neighbors\n",
    "# will give more homogeneous clusters to the cost of computation\n",
    "# time. A very large number of neighbors gives more evenly distributed\n",
    "# cluster sizes, but may not impose the local manifold structure of\n",
    "# the data\n",
    "knn_graph = kneighbors_graph(X, 30, include_self=False)\n",
    "\n",
    "for connectivity in (None, knn_graph):\n",
    "    for n_clusters in (30, 3):\n",
    "        plt.figure(figsize=(10, 4))\n",
    "        for index, linkage in enumerate((\"average\", \"complete\", \"ward\", \"single\")):\n",
    "            plt.subplot(1, 4, index + 1)\n",
    "            model = AgglomerativeClustering(\n",
    "                linkage=linkage, connectivity=connectivity, n_clusters=n_clusters\n",
    "            )\n",
    "            t0 = time.time()\n",
    "            model.fit(X)\n",
    "            elapsed_time = time.time() - t0\n",
    "            plt.scatter(X[:, 0], X[:, 1], c=model.labels_, cmap=plt.cm.nipy_spectral)\n",
    "            plt.title(\n",
    "                \"linkage=%s\\n(time %.2fs)\" % (linkage, elapsed_time),\n",
    "                fontdict=dict(verticalalignment=\"top\"),\n",
    "                y=0.9,\n",
    "            )\n",
    "            plt.axis(\"equal\")\n",
    "            plt.axis(\"off\")\n",
    "\n",
    "            plt.subplots_adjust(bottom=0, top=0.89, wspace=0, left=0, right=1)\n",
    "            plt.suptitle(\n",
    "                \"n_cluster=%i, connectivity=%r\"\n",
    "                % (n_clusters, connectivity is not None),\n",
    "                size=12,\n",
    "                y=1,\n",
    "            )\n",
    "\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "These examples are simple because the data is low dimensional. If you have high dimensional data clustering can be challenging because the data topology is not obvious.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
